{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a8ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.nn import DenseSAGEConv, dense_diff_pool, dense_mincut_pool\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "from dataset import load_pyg_dataset\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda', 1)\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24ff75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(model, optimizer, epoch, outdir):\n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        },\n",
    "        os.path.join(outdir,\n",
    "                     'best_model.pt'))\n",
    "\n",
    "def load_best_model(model, outdir):\n",
    "    checkpoint = torch.load(os.path.join(outdir, 'best_model.pt'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84862405",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling = 'mincut' #(options: diffpool, mincut)\n",
    "# dataset_name = 'Chameleon'\n",
    "\n",
    "\n",
    "\n",
    "#Pooling selector\n",
    "pooling_selector = {\n",
    "    'diffpool': dense_diff_pool,\n",
    "    'mincut': dense_mincut_pool\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 'WikiCS',\n",
    "for dataset_name in ['Cornell', 'Texas', 'Wisconsin', \"Chameleon\", \"Squirrel\", \"Crocodile\", 'Actor',\n",
    "                    'DeezerEurope', 'Cora', 'Pubmed', 'Citeseer', \n",
    "                    'Twitch-DE', 'Twitch-EN', 'Twitch-ES', 'Twitch-FR', 'Twitch-PT', 'Twitch-RU', 'WikiCS']:\n",
    "\n",
    "    #Creates outdir\n",
    "    outdir = 'results_node_clustering/{}_{}'.format(dataset_name, pooling)\n",
    "    os.makedirs(outdir, exist_ok = True)\n",
    "\n",
    "    #Loads dataset\n",
    "    data = load_pyg_dataset(\n",
    "        data_name=dataset_name,\n",
    "        device=device\n",
    "    )\n",
    "    num_clusters = data.y.max().tolist()+1\n",
    "    data.adj = to_dense_adj(data.edge_index)\n",
    "\n",
    "\n",
    "    #GNN to compute transformed node features for pooling (for assignation matrix)\n",
    "    class GNN(torch.nn.Module):\n",
    "        def __init__(self, in_channels, hidden_channels, out_channels,\n",
    "                     normalize=False, lin=True):\n",
    "            super(GNN, self).__init__()\n",
    "            self.conv1 = DenseSAGEConv(in_channels, hidden_channels, normalize)\n",
    "            self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "            self.conv2 = DenseSAGEConv(hidden_channels, hidden_channels, normalize)\n",
    "            self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "            self.conv3 = DenseSAGEConv(hidden_channels, out_channels, normalize)\n",
    "            self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "            if lin is True:\n",
    "                self.lin = torch.nn.Linear(\n",
    "                    2 * hidden_channels + out_channels,\n",
    "                    out_channels\n",
    "                )\n",
    "            else:\n",
    "                self.lin = None\n",
    "\n",
    "        def bn(self, i, x):\n",
    "            batch_size, num_nodes, num_channels = x.size()\n",
    "            x = x.view(-1, num_channels)\n",
    "            x = getattr(self, 'bn{}'.format(i))(x)\n",
    "            x = x.view(batch_size, num_nodes, num_channels)\n",
    "            return x\n",
    "\n",
    "        def forward(self, x, adj, mask=None):\n",
    "            x0 = x\n",
    "            if pooling == 'diffpool':\n",
    "                x1 = self.bn(1, F.relu(self.conv1(x0, adj, mask)))\n",
    "                x2 = self.bn(2, F.relu(self.conv2(x1, adj, mask)))\n",
    "                x3 = self.bn(3, F.relu(self.conv3(x2, adj, mask)))\n",
    "            elif pooling == 'mincut':\n",
    "                x1 = F.relu(self.conv1(x0, adj, mask))\n",
    "                x2 = F.relu(self.conv2(x1, adj, mask))\n",
    "                x3 = F.relu(self.conv3(x2, adj, mask))\n",
    "            x = torch.cat([x1, x2, x3], dim=-1)\n",
    "            if self.lin is not None:\n",
    "                x = F.relu(self.lin(x))\n",
    "            return x\n",
    "\n",
    "    #Net to compute pooling in an unsupervised manner\n",
    "    class Net(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.gnn1_pool = GNN(data.num_node_features, 64, num_clusters)\n",
    "            self.gnn1_embed = GNN(data.num_node_features, 64, 64, lin=False)\n",
    "            self.pooling = pooling_selector[pooling]\n",
    "\n",
    "        def forward(self, x, adj, mask=None):\n",
    "            s = self.gnn1_pool(x, adj, mask)\n",
    "            x = self.gnn1_embed(x, adj, mask)\n",
    "            x, adj, l1, e1 = self.pooling(x, adj, s, mask)\n",
    "            return torch.softmax(s, dim=-1), l1, e1, adj # returns assignation matrix, and auxiliary losses and new adj matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###########\n",
    "    #Training\n",
    "    ###########\n",
    "    #Optimizer, model\n",
    "    model = Net().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    #Trains using auxiliary losses only\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        s, l1, e1, adj = model(data.x, data.adj)\n",
    "        loss = l1 + e1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    #Measures mutual information of clustering computed by pooling and ground truth\n",
    "    @torch.no_grad()\n",
    "    def test():\n",
    "        model.eval()\n",
    "        pred_node_label = model(data.x, data.adj)[0].max(dim=-1)[1].detach().cpu().numpy() \n",
    "        truth_node_labels = data.y.cpu().numpy()\n",
    "        nmi = normalized_mutual_info_score(truth_node_labels.flatten(), pred_node_label.flatten())\n",
    "        return nmi\n",
    "\n",
    "\n",
    "\n",
    "    log_handle = open(osp.join(outdir, 'log.txt'), 'w')\n",
    "    best_val_acc = best_test_acc = 0\n",
    "    for epoch in range(1, 251):\n",
    "        train_loss = train(epoch)\n",
    "        val_acc = test()\n",
    "        test_acc = test()\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            save_best_model(model, optimizer, epoch, outdir)\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.8f}, '\n",
    "              f'Val NMI: {val_acc:.4f}, Test NMI: {test_acc:.4f}')\n",
    "        log_handle.write(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.8f}, '\n",
    "              f'Val NMI: {val_acc:.4f}, Test NMI: {test_acc:.4f}\\n')\n",
    "\n",
    "    print(f'Best Val NMI: {best_val_acc:.4f}, Test NMI: {best_test_acc:.4f}')\n",
    "    log_handle.write(f'\\nBest Val NMI: {best_val_acc:.4f}, Test NMI: {best_test_acc:.4f}\\n')\n",
    "    log_handle.close()\n",
    "\n",
    "\n",
    "\n",
    "    #Loads best model\n",
    "    model = load_best_model(model, outdir)\n",
    "    model.eval()\n",
    "    node_assignation = model(data.x, data.adj)[0].max(-1)[1].detach().cpu().numpy().squeeze()\n",
    "    adj_matrix = model(data.x, data.adj)[3].detach().cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    #Computes node clustering and adj matrices and saves them\n",
    "    pd.DataFrame(node_assignation).transpose().to_csv(osp.join(outdir, 'node_assignation.csv'), header = None, index=False)\n",
    "    pd.DataFrame(adj_matrix).to_csv(osp.join(outdir, 'adj_matrix.csv'), header = None, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0f2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The obtained data Cornell has 183 nodes, 298 edges, 1703 features, 5 labels, \n",
      "Epoch: 001, Train Loss: 0.16997731, Val loss: 0.1694, Test loss: 0.1694\n",
      "Epoch: 002, Train Loss: 0.16943437, Val loss: 0.1688, Test loss: 0.1688\n",
      "Epoch: 003, Train Loss: 0.16880250, Val loss: 0.1680, Test loss: 0.1680\n",
      "Epoch: 004, Train Loss: 0.16803193, Val loss: 0.1669, Test loss: 0.1669\n",
      "Epoch: 005, Train Loss: 0.16687810, Val loss: 0.1650, Test loss: 0.1650\n",
      "Epoch: 006, Train Loss: 0.16500223, Val loss: 0.1618, Test loss: 0.1618\n",
      "Epoch: 007, Train Loss: 0.16176486, Val loss: 0.1564, Test loss: 0.1564\n",
      "Epoch: 008, Train Loss: 0.15638280, Val loss: 0.1481, Test loss: 0.1481\n",
      "Epoch: 009, Train Loss: 0.14806867, Val loss: 0.1357, Test loss: 0.1357\n",
      "Epoch: 010, Train Loss: 0.13572168, Val loss: 0.1182, Test loss: 0.1182\n",
      "Epoch: 011, Train Loss: 0.11818099, Val loss: 0.0950, Test loss: 0.0950\n",
      "Epoch: 012, Train Loss: 0.09495521, Val loss: 0.0665, Test loss: 0.0665\n",
      "Epoch: 013, Train Loss: 0.06650436, Val loss: 0.0341, Test loss: 0.0341\n",
      "Epoch: 014, Train Loss: 0.03408420, Val loss: -0.0015, Test loss: -0.0015\n",
      "Epoch: 015, Train Loss: -0.00150955, Val loss: -0.0402, Test loss: -0.0402\n",
      "Epoch: 016, Train Loss: -0.04016244, Val loss: -0.0816, Test loss: -0.0816\n",
      "Epoch: 017, Train Loss: -0.08162200, Val loss: -0.1253, Test loss: -0.1253\n",
      "Epoch: 018, Train Loss: -0.12533796, Val loss: -0.1703, Test loss: -0.1703\n",
      "Epoch: 019, Train Loss: -0.17027473, Val loss: -0.2148, Test loss: -0.2148\n",
      "Epoch: 020, Train Loss: -0.21476948, Val loss: -0.2570, Test loss: -0.2570\n",
      "Epoch: 021, Train Loss: -0.25695968, Val loss: -0.2953, Test loss: -0.2953\n",
      "Epoch: 022, Train Loss: -0.29531670, Val loss: -0.3288, Test loss: -0.3288\n",
      "Epoch: 023, Train Loss: -0.32883847, Val loss: -0.3573, Test loss: -0.3573\n",
      "Epoch: 024, Train Loss: -0.35726273, Val loss: -0.3803, Test loss: -0.3803\n",
      "Epoch: 025, Train Loss: -0.38030815, Val loss: -0.3984, Test loss: -0.3984\n",
      "Epoch: 026, Train Loss: -0.39840138, Val loss: -0.4128, Test loss: -0.4128\n",
      "Epoch: 027, Train Loss: -0.41277313, Val loss: -0.4239, Test loss: -0.4239\n",
      "Epoch: 028, Train Loss: -0.42386186, Val loss: -0.4332, Test loss: -0.4332\n",
      "Epoch: 029, Train Loss: -0.43319094, Val loss: -0.4405, Test loss: -0.4405\n",
      "Epoch: 030, Train Loss: -0.44048929, Val loss: -0.4463, Test loss: -0.4463\n",
      "Epoch: 031, Train Loss: -0.44627237, Val loss: -0.4516, Test loss: -0.4516\n",
      "Epoch: 032, Train Loss: -0.45161903, Val loss: -0.4567, Test loss: -0.4567\n",
      "Epoch: 033, Train Loss: -0.45668530, Val loss: -0.4622, Test loss: -0.4622\n",
      "Epoch: 034, Train Loss: -0.46217251, Val loss: -0.4662, Test loss: -0.4662\n",
      "Epoch: 035, Train Loss: -0.46615708, Val loss: -0.4685, Test loss: -0.4685\n",
      "Epoch: 036, Train Loss: -0.46849477, Val loss: -0.4705, Test loss: -0.4705\n",
      "Epoch: 037, Train Loss: -0.47054803, Val loss: -0.4732, Test loss: -0.4732\n",
      "Epoch: 038, Train Loss: -0.47320366, Val loss: -0.4763, Test loss: -0.4763\n",
      "Epoch: 039, Train Loss: -0.47633684, Val loss: -0.4793, Test loss: -0.4793\n",
      "Epoch: 040, Train Loss: -0.47930419, Val loss: -0.4823, Test loss: -0.4823\n",
      "Epoch: 041, Train Loss: -0.48227930, Val loss: -0.4846, Test loss: -0.4846\n",
      "Epoch: 042, Train Loss: -0.48461437, Val loss: -0.4860, Test loss: -0.4860\n",
      "Epoch: 043, Train Loss: -0.48602664, Val loss: -0.4867, Test loss: -0.4867\n",
      "Epoch: 044, Train Loss: -0.48667586, Val loss: -0.4868, Test loss: -0.4868\n",
      "Epoch: 045, Train Loss: -0.48676550, Val loss: -0.4869, Test loss: -0.4869\n",
      "Epoch: 046, Train Loss: -0.48685503, Val loss: -0.4875, Test loss: -0.4875\n",
      "Epoch: 047, Train Loss: -0.48747885, Val loss: -0.4881, Test loss: -0.4881\n",
      "Epoch: 048, Train Loss: -0.48808467, Val loss: -0.4883, Test loss: -0.4883\n",
      "Epoch: 049, Train Loss: -0.48825812, Val loss: -0.4883, Test loss: -0.4883\n",
      "Epoch: 050, Train Loss: -0.48825276, Val loss: -0.4885, Test loss: -0.4885\n",
      "Epoch: 051, Train Loss: -0.48851216, Val loss: -0.4891, Test loss: -0.4891\n",
      "Epoch: 052, Train Loss: -0.48910201, Val loss: -0.4895, Test loss: -0.4895\n",
      "Epoch: 053, Train Loss: -0.48945403, Val loss: -0.4894, Test loss: -0.4894\n",
      "Epoch: 054, Train Loss: -0.48944521, Val loss: -0.4894, Test loss: -0.4894\n",
      "Epoch: 055, Train Loss: -0.48943877, Val loss: -0.4897, Test loss: -0.4897\n",
      "Epoch: 056, Train Loss: -0.48967838, Val loss: -0.4899, Test loss: -0.4899\n",
      "Epoch: 057, Train Loss: -0.48993874, Val loss: -0.4900, Test loss: -0.4900\n",
      "Epoch: 058, Train Loss: -0.48997140, Val loss: -0.4899, Test loss: -0.4899\n",
      "Epoch: 059, Train Loss: -0.48989773, Val loss: -0.4900, Test loss: -0.4900\n",
      "Epoch: 060, Train Loss: -0.48997033, Val loss: -0.4902, Test loss: -0.4902\n",
      "Epoch: 061, Train Loss: -0.49020112, Val loss: -0.4904, Test loss: -0.4904\n",
      "Epoch: 062, Train Loss: -0.49041021, Val loss: -0.4905, Test loss: -0.4905\n",
      "Epoch: 063, Train Loss: -0.49049246, Val loss: -0.4905, Test loss: -0.4905\n",
      "Epoch: 064, Train Loss: -0.49051142, Val loss: -0.4906, Test loss: -0.4906\n",
      "Epoch: 065, Train Loss: -0.49060345, Val loss: -0.4908, Test loss: -0.4908\n",
      "Epoch: 066, Train Loss: -0.49076641, Val loss: -0.4909, Test loss: -0.4909\n",
      "Epoch: 067, Train Loss: -0.49093878, Val loss: -0.4910, Test loss: -0.4910\n",
      "Epoch: 068, Train Loss: -0.49103379, Val loss: -0.4911, Test loss: -0.4911\n",
      "Epoch: 069, Train Loss: -0.49105549, Val loss: -0.4911, Test loss: -0.4911\n",
      "Epoch: 070, Train Loss: -0.49106264, Val loss: -0.4911, Test loss: -0.4911\n",
      "Epoch: 071, Train Loss: -0.49111009, Val loss: -0.4912, Test loss: -0.4912\n",
      "Epoch: 072, Train Loss: -0.49120796, Val loss: -0.4913, Test loss: -0.4913\n",
      "Epoch: 073, Train Loss: -0.49130642, Val loss: -0.4914, Test loss: -0.4914\n",
      "Epoch: 074, Train Loss: -0.49138081, Val loss: -0.4914, Test loss: -0.4914\n",
      "Epoch: 075, Train Loss: -0.49142230, Val loss: -0.4915, Test loss: -0.4915\n",
      "Epoch: 076, Train Loss: -0.49145353, Val loss: -0.4915, Test loss: -0.4915\n",
      "Epoch: 077, Train Loss: -0.49148703, Val loss: -0.4915, Test loss: -0.4915\n",
      "Epoch: 078, Train Loss: -0.49153042, Val loss: -0.4916, Test loss: -0.4916\n",
      "Epoch: 079, Train Loss: -0.49158907, Val loss: -0.4917, Test loss: -0.4917\n",
      "Epoch: 080, Train Loss: -0.49165368, Val loss: -0.4917, Test loss: -0.4917\n",
      "Epoch: 081, Train Loss: -0.49171972, Val loss: -0.4918, Test loss: -0.4918\n",
      "Epoch: 082, Train Loss: -0.49177730, Val loss: -0.4918, Test loss: -0.4918\n",
      "Epoch: 083, Train Loss: -0.49182749, Val loss: -0.4919, Test loss: -0.4919\n",
      "Epoch: 084, Train Loss: -0.49186707, Val loss: -0.4919, Test loss: -0.4919\n",
      "Epoch: 085, Train Loss: -0.49190199, Val loss: -0.4919, Test loss: -0.4919\n",
      "Epoch: 086, Train Loss: -0.49193203, Val loss: -0.4920, Test loss: -0.4920\n",
      "Epoch: 087, Train Loss: -0.49196279, Val loss: -0.4920, Test loss: -0.4920\n",
      "Epoch: 088, Train Loss: -0.49198949, Val loss: -0.4920, Test loss: -0.4920\n",
      "Epoch: 089, Train Loss: -0.49201584, Val loss: -0.4920, Test loss: -0.4920\n",
      "Epoch: 090, Train Loss: -0.49203825, Val loss: -0.4921, Test loss: -0.4921\n",
      "Epoch: 091, Train Loss: -0.49205899, Val loss: -0.4921, Test loss: -0.4921\n",
      "Epoch: 092, Train Loss: -0.49207330, Val loss: -0.4921, Test loss: -0.4921\n",
      "Epoch: 093, Train Loss: -0.49207509, Val loss: -0.4920, Test loss: -0.4920\n",
      "Epoch: 094, Train Loss: -0.49204385, Val loss: -0.4919, Test loss: -0.4919\n",
      "Epoch: 095, Train Loss: -0.49191689, Val loss: -0.4915, Test loss: -0.4915\n",
      "Epoch: 096, Train Loss: -0.49153674, Val loss: -0.4906, Test loss: -0.4906\n",
      "Epoch: 097, Train Loss: -0.49059725, Val loss: -0.4893, Test loss: -0.4893\n",
      "Epoch: 098, Train Loss: -0.48932076, Val loss: -0.4888, Test loss: -0.4888\n",
      "Epoch: 099, Train Loss: -0.48881125, Val loss: -0.4910, Test loss: -0.4910\n",
      "Epoch: 100, Train Loss: -0.49102068, Val loss: -0.4922, Test loss: -0.4922\n",
      "Epoch: 101, Train Loss: -0.49215794, Val loss: -0.4907, Test loss: -0.4907\n",
      "Epoch: 102, Train Loss: -0.49073374, Val loss: -0.4904, Test loss: -0.4904\n",
      "Epoch: 103, Train Loss: -0.49044025, Val loss: -0.4919, Test loss: -0.4919\n",
      "Epoch: 104, Train Loss: -0.49191439, Val loss: -0.4919, Test loss: -0.4919\n",
      "Epoch: 105, Train Loss: -0.49187005, Val loss: -0.4909, Test loss: -0.4909\n",
      "Epoch: 106, Train Loss: -0.49089944, Val loss: -0.4916, Test loss: -0.4916\n",
      "Epoch: 107, Train Loss: -0.49163806, Val loss: -0.4922, Test loss: -0.4922\n",
      "Epoch: 108, Train Loss: -0.49218464, Val loss: -0.4914, Test loss: -0.4914\n",
      "Epoch: 109, Train Loss: -0.49137664, Val loss: -0.4916, Test loss: -0.4916\n",
      "Epoch: 110, Train Loss: -0.49160278, Val loss: -0.4923, Test loss: -0.4923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 111, Train Loss: -0.49225080, Val loss: -0.4917, Test loss: -0.4917\n",
      "Epoch: 112, Train Loss: -0.49170339, Val loss: -0.4917, Test loss: -0.4917\n",
      "Epoch: 113, Train Loss: -0.49165833, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 114, Train Loss: -0.49225581, Val loss: -0.4919, Test loss: -0.4919\n",
      "Epoch: 115, Train Loss: -0.49192071, Val loss: -0.4918, Test loss: -0.4918\n",
      "Epoch: 116, Train Loss: -0.49175072, Val loss: -0.4922, Test loss: -0.4922\n",
      "Epoch: 117, Train Loss: -0.49223995, Val loss: -0.4921, Test loss: -0.4921\n",
      "Epoch: 118, Train Loss: -0.49208617, Val loss: -0.4919, Test loss: -0.4919\n",
      "Epoch: 119, Train Loss: -0.49187052, Val loss: -0.4922, Test loss: -0.4922\n",
      "Epoch: 120, Train Loss: -0.49222004, Val loss: -0.4922, Test loss: -0.4922\n",
      "Epoch: 121, Train Loss: -0.49220693, Val loss: -0.4920, Test loss: -0.4920\n",
      "Epoch: 122, Train Loss: -0.49198258, Val loss: -0.4922, Test loss: -0.4922\n",
      "Epoch: 123, Train Loss: -0.49220240, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 124, Train Loss: -0.49228108, Val loss: -0.4921, Test loss: -0.4921\n",
      "Epoch: 125, Train Loss: -0.49208987, Val loss: -0.4922, Test loss: -0.4922\n",
      "Epoch: 126, Train Loss: -0.49218822, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 127, Train Loss: -0.49231672, Val loss: -0.4922, Test loss: -0.4922\n",
      "Epoch: 128, Train Loss: -0.49219763, Val loss: -0.4922, Test loss: -0.4922\n",
      "Epoch: 129, Train Loss: -0.49219227, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 130, Train Loss: -0.49232066, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 131, Train Loss: -0.49228263, Val loss: -0.4922, Test loss: -0.4922\n",
      "Epoch: 132, Train Loss: -0.49222064, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 133, Train Loss: -0.49230206, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 134, Train Loss: -0.49233377, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 135, Train Loss: -0.49227035, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 136, Train Loss: -0.49228573, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 137, Train Loss: -0.49234724, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 138, Train Loss: -0.49232459, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 139, Train Loss: -0.49229431, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 140, Train Loss: -0.49233484, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 141, Train Loss: -0.49235868, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 142, Train Loss: -0.49233091, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 143, Train Loss: -0.49232662, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 144, Train Loss: -0.49235952, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 145, Train Loss: -0.49236643, Val loss: -0.4923, Test loss: -0.4923\n",
      "Epoch: 146, Train Loss: -0.49234676, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 147, Train Loss: -0.49235034, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 148, Train Loss: -0.49237347, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 149, Train Loss: -0.49237633, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 150, Train Loss: -0.49236369, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 151, Train Loss: -0.49236691, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 152, Train Loss: -0.49238241, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 153, Train Loss: -0.49238718, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 154, Train Loss: -0.49237943, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 155, Train Loss: -0.49237955, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 156, Train Loss: -0.49239004, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 157, Train Loss: -0.49239671, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 158, Train Loss: -0.49239397, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 159, Train Loss: -0.49239147, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 160, Train Loss: -0.49239576, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 161, Train Loss: -0.49240315, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 162, Train Loss: -0.49240577, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 163, Train Loss: -0.49240410, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 164, Train Loss: -0.49240386, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 165, Train Loss: -0.49240792, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 166, Train Loss: -0.49241316, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 167, Train Loss: -0.49241555, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 168, Train Loss: -0.49241519, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 169, Train Loss: -0.49241555, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 170, Train Loss: -0.49241757, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 171, Train Loss: -0.49242115, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 172, Train Loss: -0.49242425, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 173, Train Loss: -0.49242628, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 174, Train Loss: -0.49242699, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 175, Train Loss: -0.49242771, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 176, Train Loss: -0.49242997, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 177, Train Loss: -0.49243152, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 178, Train Loss: -0.49243331, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 179, Train Loss: -0.49243474, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 180, Train Loss: -0.49243796, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 181, Train Loss: -0.49244082, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 182, Train Loss: -0.49244344, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 183, Train Loss: -0.49244559, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 184, Train Loss: -0.49244761, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 185, Train Loss: -0.49244928, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 186, Train Loss: -0.49245155, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 187, Train Loss: -0.49245417, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 188, Train Loss: -0.49245632, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 189, Train Loss: -0.49245918, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 190, Train Loss: -0.49246264, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 191, Train Loss: -0.49246609, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 192, Train Loss: -0.49247003, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 193, Train Loss: -0.49247432, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 194, Train Loss: -0.49247873, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 195, Train Loss: -0.49248397, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 196, Train Loss: -0.49248970, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 197, Train Loss: -0.49249601, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 198, Train Loss: -0.49250329, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 199, Train Loss: -0.49251151, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 200, Train Loss: -0.49252069, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 201, Train Loss: -0.49253201, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 202, Train Loss: -0.49254465, Val loss: -0.4926, Test loss: -0.4926\n",
      "Epoch: 203, Train Loss: -0.49255776, Val loss: -0.4926, Test loss: -0.4926\n",
      "Epoch: 204, Train Loss: -0.49257123, Val loss: -0.4926, Test loss: -0.4926\n",
      "Epoch: 205, Train Loss: -0.49258220, Val loss: -0.4926, Test loss: -0.4926\n",
      "Epoch: 206, Train Loss: -0.49258423, Val loss: -0.4926, Test loss: -0.4926\n",
      "Epoch: 207, Train Loss: -0.49256694, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 208, Train Loss: -0.49250162, Val loss: -0.4924, Test loss: -0.4924\n",
      "Epoch: 209, Train Loss: -0.49235654, Val loss: -0.4920, Test loss: -0.4920\n",
      "Epoch: 210, Train Loss: -0.49203789, Val loss: -0.4917, Test loss: -0.4917\n",
      "Epoch: 211, Train Loss: -0.49167204, Val loss: -0.4914, Test loss: -0.4914\n",
      "Epoch: 212, Train Loss: -0.49141479, Val loss: -0.4925, Test loss: -0.4925\n",
      "Epoch: 213, Train Loss: -0.49252093, Val loss: -0.4950, Test loss: -0.4950\n",
      "Epoch: 214, Train Loss: -0.49504054, Val loss: -0.4979, Test loss: -0.4979\n",
      "Epoch: 215, Train Loss: -0.49788666, Val loss: -0.4985, Test loss: -0.4985\n",
      "Epoch: 216, Train Loss: -0.49848557, Val loss: -0.4967, Test loss: -0.4967\n",
      "Epoch: 217, Train Loss: -0.49671590, Val loss: -0.4957, Test loss: -0.4957\n",
      "Epoch: 218, Train Loss: -0.49568069, Val loss: -0.4961, Test loss: -0.4961\n",
      "Epoch: 219, Train Loss: -0.49611413, Val loss: -0.4975, Test loss: -0.4975\n",
      "Epoch: 220, Train Loss: -0.49752450, Val loss: -0.4980, Test loss: -0.4980\n",
      "Epoch: 221, Train Loss: -0.49798846, Val loss: -0.4974, Test loss: -0.4974\n",
      "Epoch: 222, Train Loss: -0.49741232, Val loss: -0.4973, Test loss: -0.4973\n",
      "Epoch: 223, Train Loss: -0.49730122, Val loss: -0.4981, Test loss: -0.4981\n",
      "Epoch: 224, Train Loss: -0.49811244, Val loss: -0.4986, Test loss: -0.4986\n",
      "Epoch: 225, Train Loss: -0.49861658, Val loss: -0.4981, Test loss: -0.4981\n",
      "Epoch: 226, Train Loss: -0.49809623, Val loss: -0.4977, Test loss: -0.4977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 227, Train Loss: -0.49768841, Val loss: -0.4983, Test loss: -0.4983\n",
      "Epoch: 228, Train Loss: -0.49828494, Val loss: -0.4989, Test loss: -0.4989\n",
      "Epoch: 229, Train Loss: -0.49891472, Val loss: -0.4987, Test loss: -0.4987\n",
      "Epoch: 230, Train Loss: -0.49873090, Val loss: -0.4984, Test loss: -0.4984\n",
      "Epoch: 231, Train Loss: -0.49835253, Val loss: -0.4984, Test loss: -0.4984\n",
      "Epoch: 232, Train Loss: -0.49843836, Val loss: -0.4987, Test loss: -0.4987\n",
      "Epoch: 233, Train Loss: -0.49874806, Val loss: -0.4988, Test loss: -0.4988\n",
      "Epoch: 234, Train Loss: -0.49876761, Val loss: -0.4987, Test loss: -0.4987\n",
      "Epoch: 235, Train Loss: -0.49868071, Val loss: -0.4988, Test loss: -0.4988\n",
      "Epoch: 236, Train Loss: -0.49877739, Val loss: -0.4989, Test loss: -0.4989\n",
      "Epoch: 237, Train Loss: -0.49890065, Val loss: -0.4988, Test loss: -0.4988\n",
      "Epoch: 238, Train Loss: -0.49884391, Val loss: -0.4988, Test loss: -0.4988\n",
      "Epoch: 239, Train Loss: -0.49879229, Val loss: -0.4989, Test loss: -0.4989\n",
      "Epoch: 240, Train Loss: -0.49888611, Val loss: -0.4990, Test loss: -0.4990\n",
      "Epoch: 241, Train Loss: -0.49897242, Val loss: -0.4989, Test loss: -0.4989\n",
      "Epoch: 242, Train Loss: -0.49893725, Val loss: -0.4989, Test loss: -0.4989\n",
      "Epoch: 243, Train Loss: -0.49892390, Val loss: -0.4990, Test loss: -0.4990\n",
      "Epoch: 244, Train Loss: -0.49900842, Val loss: -0.4990, Test loss: -0.4990\n",
      "Epoch: 245, Train Loss: -0.49903727, Val loss: -0.4990, Test loss: -0.4990\n",
      "Epoch: 246, Train Loss: -0.49897122, Val loss: -0.4989, Test loss: -0.4989\n",
      "Epoch: 247, Train Loss: -0.49893999, Val loss: -0.4990, Test loss: -0.4990\n",
      "Epoch: 248, Train Loss: -0.49901509, Val loss: -0.4991, Test loss: -0.4991\n",
      "Epoch: 249, Train Loss: -0.49907565, Val loss: -0.4990, Test loss: -0.4990\n",
      "Epoch: 250, Train Loss: -0.49904895, Val loss: -0.4990, Test loss: -0.4990\n",
      "Best Val loss: -0.4991, Test loss: -0.4991\n",
      "The obtained data Texas has 183 nodes, 325 edges, 1703 features, 5 labels, \n",
      "Epoch: 001, Train Loss: 0.16978270, Val loss: 0.1692, Test loss: 0.1692\n",
      "Epoch: 002, Train Loss: 0.16923213, Val loss: 0.1682, Test loss: 0.1682\n",
      "Epoch: 003, Train Loss: 0.16821980, Val loss: 0.1657, Test loss: 0.1657\n",
      "Epoch: 004, Train Loss: 0.16573894, Val loss: 0.1602, Test loss: 0.1602\n",
      "Epoch: 005, Train Loss: 0.16017663, Val loss: 0.1491, Test loss: 0.1491\n",
      "Epoch: 006, Train Loss: 0.14913547, Val loss: 0.1312, Test loss: 0.1312\n",
      "Epoch: 007, Train Loss: 0.13124919, Val loss: 0.1103, Test loss: 0.1103\n",
      "Epoch: 008, Train Loss: 0.11031210, Val loss: 0.0935, Test loss: 0.0935\n",
      "Epoch: 009, Train Loss: 0.09345162, Val loss: 0.0781, Test loss: 0.0781\n",
      "Epoch: 010, Train Loss: 0.07806528, Val loss: 0.0592, Test loss: 0.0592\n",
      "Epoch: 011, Train Loss: 0.05922663, Val loss: 0.0390, Test loss: 0.0390\n",
      "Epoch: 012, Train Loss: 0.03895330, Val loss: 0.0190, Test loss: 0.0190\n",
      "Epoch: 013, Train Loss: 0.01898456, Val loss: -0.0032, Test loss: -0.0032\n",
      "Epoch: 014, Train Loss: -0.00318480, Val loss: -0.0284, Test loss: -0.0284\n",
      "Epoch: 015, Train Loss: -0.02841604, Val loss: -0.0542, Test loss: -0.0542\n",
      "Epoch: 016, Train Loss: -0.05415618, Val loss: -0.0804, Test loss: -0.0804\n",
      "Epoch: 017, Train Loss: -0.08037961, Val loss: -0.1091, Test loss: -0.1091\n",
      "Epoch: 018, Train Loss: -0.10905659, Val loss: -0.1390, Test loss: -0.1390\n",
      "Epoch: 019, Train Loss: -0.13898575, Val loss: -0.1676, Test loss: -0.1676\n",
      "Epoch: 020, Train Loss: -0.16757333, Val loss: -0.1956, Test loss: -0.1956\n",
      "Epoch: 021, Train Loss: -0.19555628, Val loss: -0.2213, Test loss: -0.2213\n",
      "Epoch: 022, Train Loss: -0.22132409, Val loss: -0.2444, Test loss: -0.2444\n",
      "Epoch: 023, Train Loss: -0.24439251, Val loss: -0.2662, Test loss: -0.2662\n",
      "Epoch: 024, Train Loss: -0.26616657, Val loss: -0.2841, Test loss: -0.2841\n",
      "Epoch: 025, Train Loss: -0.28407621, Val loss: -0.3001, Test loss: -0.3001\n",
      "Epoch: 026, Train Loss: -0.30010521, Val loss: -0.3125, Test loss: -0.3125\n",
      "Epoch: 027, Train Loss: -0.31249297, Val loss: -0.3232, Test loss: -0.3232\n",
      "Epoch: 028, Train Loss: -0.32316291, Val loss: -0.3303, Test loss: -0.3303\n",
      "Epoch: 029, Train Loss: -0.33027256, Val loss: -0.3363, Test loss: -0.3363\n",
      "Epoch: 030, Train Loss: -0.33634412, Val loss: -0.3411, Test loss: -0.3411\n",
      "Epoch: 031, Train Loss: -0.34114039, Val loss: -0.3451, Test loss: -0.3451\n",
      "Epoch: 032, Train Loss: -0.34506643, Val loss: -0.3492, Test loss: -0.3492\n",
      "Epoch: 033, Train Loss: -0.34915018, Val loss: -0.3521, Test loss: -0.3521\n",
      "Epoch: 034, Train Loss: -0.35207081, Val loss: -0.3548, Test loss: -0.3548\n",
      "Epoch: 035, Train Loss: -0.35478783, Val loss: -0.3569, Test loss: -0.3569\n",
      "Epoch: 036, Train Loss: -0.35687065, Val loss: -0.3582, Test loss: -0.3582\n",
      "Epoch: 037, Train Loss: -0.35815489, Val loss: -0.3598, Test loss: -0.3598\n",
      "Epoch: 038, Train Loss: -0.35975158, Val loss: -0.3610, Test loss: -0.3610\n",
      "Epoch: 039, Train Loss: -0.36103487, Val loss: -0.3623, Test loss: -0.3623\n",
      "Epoch: 040, Train Loss: -0.36229169, Val loss: -0.3637, Test loss: -0.3637\n",
      "Epoch: 041, Train Loss: -0.36374259, Val loss: -0.3648, Test loss: -0.3648\n",
      "Epoch: 042, Train Loss: -0.36482859, Val loss: -0.3658, Test loss: -0.3658\n",
      "Epoch: 043, Train Loss: -0.36581719, Val loss: -0.3668, Test loss: -0.3668\n",
      "Epoch: 044, Train Loss: -0.36675811, Val loss: -0.3673, Test loss: -0.3673\n",
      "Epoch: 045, Train Loss: -0.36728084, Val loss: -0.3677, Test loss: -0.3677\n",
      "Epoch: 046, Train Loss: -0.36768162, Val loss: -0.3682, Test loss: -0.3682\n",
      "Epoch: 047, Train Loss: -0.36817026, Val loss: -0.3685, Test loss: -0.3685\n",
      "Epoch: 048, Train Loss: -0.36851192, Val loss: -0.3688, Test loss: -0.3688\n",
      "Epoch: 049, Train Loss: -0.36878610, Val loss: -0.3692, Test loss: -0.3692\n",
      "Epoch: 050, Train Loss: -0.36917174, Val loss: -0.3695, Test loss: -0.3695\n",
      "Epoch: 051, Train Loss: -0.36952138, Val loss: -0.3698, Test loss: -0.3698\n",
      "Epoch: 052, Train Loss: -0.36979091, Val loss: -0.3701, Test loss: -0.3701\n",
      "Epoch: 053, Train Loss: -0.37011731, Val loss: -0.3704, Test loss: -0.3704\n",
      "Epoch: 054, Train Loss: -0.37043548, Val loss: -0.3707, Test loss: -0.3707\n",
      "Epoch: 055, Train Loss: -0.37065089, Val loss: -0.3709, Test loss: -0.3709\n",
      "Epoch: 056, Train Loss: -0.37089372, Val loss: -0.3712, Test loss: -0.3712\n",
      "Epoch: 057, Train Loss: -0.37124658, Val loss: -0.3716, Test loss: -0.3716\n",
      "Epoch: 058, Train Loss: -0.37162805, Val loss: -0.3720, Test loss: -0.3720\n",
      "Epoch: 059, Train Loss: -0.37203658, Val loss: -0.3726, Test loss: -0.3726\n",
      "Epoch: 060, Train Loss: -0.37256467, Val loss: -0.3732, Test loss: -0.3732\n",
      "Epoch: 061, Train Loss: -0.37317181, Val loss: -0.3738, Test loss: -0.3738\n",
      "Epoch: 062, Train Loss: -0.37382388, Val loss: -0.3745, Test loss: -0.3745\n",
      "Epoch: 063, Train Loss: -0.37448943, Val loss: -0.3751, Test loss: -0.3751\n",
      "Epoch: 064, Train Loss: -0.37510061, Val loss: -0.3755, Test loss: -0.3755\n",
      "Epoch: 065, Train Loss: -0.37553394, Val loss: -0.3757, Test loss: -0.3757\n",
      "Epoch: 066, Train Loss: -0.37573647, Val loss: -0.3758, Test loss: -0.3758\n",
      "Epoch: 067, Train Loss: -0.37578785, Val loss: -0.3758, Test loss: -0.3758\n",
      "Epoch: 068, Train Loss: -0.37579298, Val loss: -0.3758, Test loss: -0.3758\n",
      "Epoch: 069, Train Loss: -0.37580597, Val loss: -0.3758, Test loss: -0.3758\n",
      "Epoch: 070, Train Loss: -0.37583899, Val loss: -0.3759, Test loss: -0.3759\n",
      "Epoch: 071, Train Loss: -0.37588859, Val loss: -0.3759, Test loss: -0.3759\n",
      "Epoch: 072, Train Loss: -0.37592185, Val loss: -0.3760, Test loss: -0.3760\n",
      "Epoch: 073, Train Loss: -0.37595046, Val loss: -0.3760, Test loss: -0.3760\n",
      "Epoch: 074, Train Loss: -0.37598443, Val loss: -0.3760, Test loss: -0.3760\n",
      "Epoch: 075, Train Loss: -0.37602651, Val loss: -0.3761, Test loss: -0.3761\n",
      "Epoch: 076, Train Loss: -0.37607348, Val loss: -0.3761, Test loss: -0.3761\n",
      "Epoch: 077, Train Loss: -0.37611675, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 078, Train Loss: -0.37615085, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 079, Train Loss: -0.37617731, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 080, Train Loss: -0.37619841, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 081, Train Loss: -0.37621629, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 082, Train Loss: -0.37623429, Val loss: -0.3763, Test loss: -0.3763\n",
      "Epoch: 083, Train Loss: -0.37625289, Val loss: -0.3763, Test loss: -0.3763\n",
      "Epoch: 084, Train Loss: -0.37626958, Val loss: -0.3763, Test loss: -0.3763\n",
      "Epoch: 085, Train Loss: -0.37628317, Val loss: -0.3763, Test loss: -0.3763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 086, Train Loss: -0.37628877, Val loss: -0.3763, Test loss: -0.3763\n",
      "Epoch: 087, Train Loss: -0.37627757, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 088, Train Loss: -0.37623882, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 089, Train Loss: -0.37615621, Val loss: -0.3760, Test loss: -0.3760\n",
      "Epoch: 090, Train Loss: -0.37601244, Val loss: -0.3759, Test loss: -0.3759\n",
      "Epoch: 091, Train Loss: -0.37585485, Val loss: -0.3758, Test loss: -0.3758\n",
      "Epoch: 092, Train Loss: -0.37584507, Val loss: -0.3761, Test loss: -0.3761\n",
      "Epoch: 093, Train Loss: -0.37608707, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 094, Train Loss: -0.37637568, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 095, Train Loss: -0.37642801, Val loss: -0.3763, Test loss: -0.3763\n",
      "Epoch: 096, Train Loss: -0.37626028, Val loss: -0.3761, Test loss: -0.3761\n",
      "Epoch: 097, Train Loss: -0.37613571, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 098, Train Loss: -0.37624633, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 099, Train Loss: -0.37643862, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 100, Train Loss: -0.37648273, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 101, Train Loss: -0.37637472, Val loss: -0.3763, Test loss: -0.3763\n",
      "Epoch: 102, Train Loss: -0.37629986, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 103, Train Loss: -0.37637913, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 104, Train Loss: -0.37649858, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 105, Train Loss: -0.37651980, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 106, Train Loss: -0.37645018, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 107, Train Loss: -0.37640345, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 108, Train Loss: -0.37644792, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 109, Train Loss: -0.37652576, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 110, Train Loss: -0.37655604, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 111, Train Loss: -0.37652516, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 112, Train Loss: -0.37648571, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 113, Train Loss: -0.37648916, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 114, Train Loss: -0.37653124, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 115, Train Loss: -0.37657297, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 116, Train Loss: -0.37658525, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 117, Train Loss: -0.37656796, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 118, Train Loss: -0.37654650, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 119, Train Loss: -0.37654006, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 120, Train Loss: -0.37655485, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 121, Train Loss: -0.37658072, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 122, Train Loss: -0.37660277, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 123, Train Loss: -0.37661350, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 124, Train Loss: -0.37661195, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 125, Train Loss: -0.37660265, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 126, Train Loss: -0.37659264, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 127, Train Loss: -0.37658620, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 128, Train Loss: -0.37658417, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 129, Train Loss: -0.37658620, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 130, Train Loss: -0.37659061, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 131, Train Loss: -0.37659717, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 132, Train Loss: -0.37660277, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 133, Train Loss: -0.37660837, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 134, Train Loss: -0.37661147, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 135, Train Loss: -0.37661338, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 136, Train Loss: -0.37661171, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 137, Train Loss: -0.37660503, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 138, Train Loss: -0.37659168, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 139, Train Loss: -0.37656701, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 140, Train Loss: -0.37652564, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 141, Train Loss: -0.37645912, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 142, Train Loss: -0.37635744, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 143, Train Loss: -0.37623024, Val loss: -0.3761, Test loss: -0.3761\n",
      "Epoch: 144, Train Loss: -0.37609780, Val loss: -0.3761, Test loss: -0.3761\n",
      "Epoch: 145, Train Loss: -0.37607050, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 146, Train Loss: -0.37616992, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 147, Train Loss: -0.37640989, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 148, Train Loss: -0.37661588, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 149, Train Loss: -0.37667429, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 150, Train Loss: -0.37658608, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 151, Train Loss: -0.37644768, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 152, Train Loss: -0.37638283, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 153, Train Loss: -0.37642968, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 154, Train Loss: -0.37656200, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 155, Train Loss: -0.37666643, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 156, Train Loss: -0.37668061, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 157, Train Loss: -0.37661922, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 158, Train Loss: -0.37654579, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 159, Train Loss: -0.37652230, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 160, Train Loss: -0.37655997, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 161, Train Loss: -0.37663102, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 162, Train Loss: -0.37668419, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 163, Train Loss: -0.37669396, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 164, Train Loss: -0.37666571, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 165, Train Loss: -0.37662697, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 166, Train Loss: -0.37660468, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 167, Train Loss: -0.37660813, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 168, Train Loss: -0.37663555, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 169, Train Loss: -0.37666988, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 170, Train Loss: -0.37669563, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 171, Train Loss: -0.37670529, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 172, Train Loss: -0.37669957, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 173, Train Loss: -0.37668455, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 174, Train Loss: -0.37666798, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 175, Train Loss: -0.37665546, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 176, Train Loss: -0.37664807, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 177, Train Loss: -0.37664890, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 178, Train Loss: -0.37665379, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 179, Train Loss: -0.37666261, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 180, Train Loss: -0.37667251, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 181, Train Loss: -0.37668216, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 182, Train Loss: -0.37669075, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 183, Train Loss: -0.37669754, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 184, Train Loss: -0.37670255, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 185, Train Loss: -0.37670672, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 186, Train Loss: -0.37670875, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 187, Train Loss: -0.37671018, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 188, Train Loss: -0.37671018, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 189, Train Loss: -0.37670887, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 190, Train Loss: -0.37670517, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 191, Train Loss: -0.37669766, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 192, Train Loss: -0.37668335, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 193, Train Loss: -0.37665677, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 194, Train Loss: -0.37660968, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 195, Train Loss: -0.37652493, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 196, Train Loss: -0.37638354, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 197, Train Loss: -0.37616789, Val loss: -0.3759, Test loss: -0.3759\n",
      "Epoch: 198, Train Loss: -0.37587953, Val loss: -0.3756, Test loss: -0.3756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199, Train Loss: -0.37563753, Val loss: -0.3756, Test loss: -0.3756\n",
      "Epoch: 200, Train Loss: -0.37562335, Val loss: -0.3760, Test loss: -0.3760\n",
      "Epoch: 201, Train Loss: -0.37602043, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 202, Train Loss: -0.37650895, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 203, Train Loss: -0.37672925, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 204, Train Loss: -0.37656403, Val loss: -0.3763, Test loss: -0.3763\n",
      "Epoch: 205, Train Loss: -0.37626600, Val loss: -0.3762, Test loss: -0.3762\n",
      "Epoch: 206, Train Loss: -0.37619579, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 207, Train Loss: -0.37640727, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 208, Train Loss: -0.37667668, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 209, Train Loss: -0.37671208, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 210, Train Loss: -0.37654221, Val loss: -0.3764, Test loss: -0.3764\n",
      "Epoch: 211, Train Loss: -0.37641549, Val loss: -0.3765, Test loss: -0.3765\n",
      "Epoch: 212, Train Loss: -0.37647927, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 213, Train Loss: -0.37665594, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 214, Train Loss: -0.37673342, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 215, Train Loss: -0.37666070, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 216, Train Loss: -0.37656021, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 217, Train Loss: -0.37655914, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 218, Train Loss: -0.37665498, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 219, Train Loss: -0.37673092, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 220, Train Loss: -0.37671769, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 221, Train Loss: -0.37665451, Val loss: -0.3766, Test loss: -0.3766\n",
      "Epoch: 222, Train Loss: -0.37662363, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 223, Train Loss: -0.37665856, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 224, Train Loss: -0.37671578, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 225, Train Loss: -0.37673879, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 226, Train Loss: -0.37671566, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 227, Train Loss: -0.37668097, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 228, Train Loss: -0.37667429, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 229, Train Loss: -0.37669921, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 230, Train Loss: -0.37673080, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 231, Train Loss: -0.37674105, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 232, Train Loss: -0.37672806, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 233, Train Loss: -0.37670958, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 234, Train Loss: -0.37670302, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 235, Train Loss: -0.37671340, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 236, Train Loss: -0.37673128, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 237, Train Loss: -0.37674248, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 238, Train Loss: -0.37674201, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 239, Train Loss: -0.37673378, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 240, Train Loss: -0.37672567, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 241, Train Loss: -0.37672329, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 242, Train Loss: -0.37672782, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 243, Train Loss: -0.37673604, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 244, Train Loss: -0.37674379, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 245, Train Loss: -0.37674594, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 246, Train Loss: -0.37674534, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 247, Train Loss: -0.37674129, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 248, Train Loss: -0.37673736, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 249, Train Loss: -0.37673593, Val loss: -0.3767, Test loss: -0.3767\n",
      "Epoch: 250, Train Loss: -0.37673628, Val loss: -0.3767, Test loss: -0.3767\n",
      "Best Val loss: -0.3767, Test loss: -0.3767\n",
      "The obtained data Wisconsin has 251 nodes, 515 edges, 1703 features, 5 labels, \n",
      "Epoch: 001, Train Loss: 0.20246035, Val loss: 0.2021, Test loss: 0.2021\n",
      "Epoch: 002, Train Loss: 0.20208222, Val loss: 0.2009, Test loss: 0.2009\n",
      "Epoch: 003, Train Loss: 0.20085394, Val loss: 0.1970, Test loss: 0.1970\n",
      "Epoch: 004, Train Loss: 0.19697559, Val loss: 0.1881, Test loss: 0.1881\n",
      "Epoch: 005, Train Loss: 0.18814087, Val loss: 0.1715, Test loss: 0.1715\n",
      "Epoch: 006, Train Loss: 0.17148006, Val loss: 0.1508, Test loss: 0.1508\n",
      "Epoch: 007, Train Loss: 0.15079224, Val loss: 0.1333, Test loss: 0.1333\n",
      "Epoch: 008, Train Loss: 0.13328099, Val loss: 0.1168, Test loss: 0.1168\n",
      "Epoch: 009, Train Loss: 0.11677003, Val loss: 0.0999, Test loss: 0.0999\n",
      "Epoch: 010, Train Loss: 0.09988487, Val loss: 0.0830, Test loss: 0.0830\n",
      "Epoch: 011, Train Loss: 0.08296239, Val loss: 0.0651, Test loss: 0.0651\n",
      "Epoch: 012, Train Loss: 0.06509125, Val loss: 0.0474, Test loss: 0.0474\n",
      "Epoch: 013, Train Loss: 0.04744864, Val loss: 0.0299, Test loss: 0.0299\n",
      "Epoch: 014, Train Loss: 0.02985966, Val loss: 0.0112, Test loss: 0.0112\n",
      "Epoch: 015, Train Loss: 0.01117086, Val loss: -0.0075, Test loss: -0.0075\n",
      "Epoch: 016, Train Loss: -0.00747871, Val loss: -0.0252, Test loss: -0.0252\n",
      "Epoch: 017, Train Loss: -0.02524507, Val loss: -0.0426, Test loss: -0.0426\n",
      "Epoch: 018, Train Loss: -0.04263449, Val loss: -0.0594, Test loss: -0.0594\n",
      "Epoch: 019, Train Loss: -0.05936289, Val loss: -0.0759, Test loss: -0.0759\n",
      "Epoch: 020, Train Loss: -0.07593632, Val loss: -0.0922, Test loss: -0.0922\n",
      "Epoch: 021, Train Loss: -0.09221125, Val loss: -0.1090, Test loss: -0.1090\n",
      "Epoch: 022, Train Loss: -0.10897601, Val loss: -0.1260, Test loss: -0.1260\n",
      "Epoch: 023, Train Loss: -0.12598622, Val loss: -0.1422, Test loss: -0.1422\n",
      "Epoch: 024, Train Loss: -0.14224219, Val loss: -0.1568, Test loss: -0.1568\n",
      "Epoch: 025, Train Loss: -0.15677810, Val loss: -0.1706, Test loss: -0.1706\n",
      "Epoch: 026, Train Loss: -0.17063427, Val loss: -0.1837, Test loss: -0.1837\n",
      "Epoch: 027, Train Loss: -0.18365037, Val loss: -0.1954, Test loss: -0.1954\n",
      "Epoch: 028, Train Loss: -0.19538093, Val loss: -0.2055, Test loss: -0.2055\n",
      "Epoch: 029, Train Loss: -0.20550525, Val loss: -0.2140, Test loss: -0.2140\n",
      "Epoch: 030, Train Loss: -0.21403515, Val loss: -0.2207, Test loss: -0.2207\n",
      "Epoch: 031, Train Loss: -0.22072327, Val loss: -0.2266, Test loss: -0.2266\n",
      "Epoch: 032, Train Loss: -0.22664905, Val loss: -0.2318, Test loss: -0.2318\n",
      "Epoch: 033, Train Loss: -0.23182440, Val loss: -0.2364, Test loss: -0.2364\n",
      "Epoch: 034, Train Loss: -0.23640478, Val loss: -0.2401, Test loss: -0.2401\n",
      "Epoch: 035, Train Loss: -0.24010026, Val loss: -0.2430, Test loss: -0.2430\n",
      "Epoch: 036, Train Loss: -0.24297488, Val loss: -0.2451, Test loss: -0.2451\n",
      "Epoch: 037, Train Loss: -0.24508357, Val loss: -0.2469, Test loss: -0.2469\n",
      "Epoch: 038, Train Loss: -0.24686480, Val loss: -0.2479, Test loss: -0.2479\n",
      "Epoch: 039, Train Loss: -0.24789488, Val loss: -0.2502, Test loss: -0.2502\n",
      "Epoch: 040, Train Loss: -0.25018620, Val loss: -0.2531, Test loss: -0.2531\n",
      "Epoch: 041, Train Loss: -0.25310838, Val loss: -0.2544, Test loss: -0.2544\n",
      "Epoch: 042, Train Loss: -0.25443923, Val loss: -0.2544, Test loss: -0.2544\n",
      "Epoch: 043, Train Loss: -0.25443864, Val loss: -0.2555, Test loss: -0.2555\n",
      "Epoch: 044, Train Loss: -0.25550842, Val loss: -0.2574, Test loss: -0.2574\n",
      "Epoch: 045, Train Loss: -0.25735319, Val loss: -0.2575, Test loss: -0.2575\n",
      "Epoch: 046, Train Loss: -0.25746906, Val loss: -0.2577, Test loss: -0.2577\n",
      "Epoch: 047, Train Loss: -0.25773275, Val loss: -0.2595, Test loss: -0.2595\n",
      "Epoch: 048, Train Loss: -0.25954306, Val loss: -0.2601, Test loss: -0.2601\n",
      "Epoch: 049, Train Loss: -0.26014698, Val loss: -0.2600, Test loss: -0.2600\n",
      "Epoch: 050, Train Loss: -0.26001906, Val loss: -0.2611, Test loss: -0.2611\n",
      "Epoch: 051, Train Loss: -0.26110566, Val loss: -0.2617, Test loss: -0.2617\n",
      "Epoch: 052, Train Loss: -0.26169538, Val loss: -0.2615, Test loss: -0.2615\n",
      "Epoch: 053, Train Loss: -0.26149344, Val loss: -0.2621, Test loss: -0.2621\n",
      "Epoch: 054, Train Loss: -0.26214778, Val loss: -0.2627, Test loss: -0.2627\n",
      "Epoch: 055, Train Loss: -0.26272297, Val loss: -0.2626, Test loss: -0.2626\n",
      "Epoch: 056, Train Loss: -0.26260972, Val loss: -0.2629, Test loss: -0.2629\n",
      "Epoch: 057, Train Loss: -0.26293254, Val loss: -0.2634, Test loss: -0.2634\n",
      "Epoch: 058, Train Loss: -0.26341867, Val loss: -0.2634, Test loss: -0.2634\n",
      "Epoch: 059, Train Loss: -0.26340890, Val loss: -0.2635, Test loss: -0.2635\n",
      "Epoch: 060, Train Loss: -0.26352608, Val loss: -0.2639, Test loss: -0.2639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 061, Train Loss: -0.26393819, Val loss: -0.2641, Test loss: -0.2641\n",
      "Epoch: 062, Train Loss: -0.26408994, Val loss: -0.2642, Test loss: -0.2642\n",
      "Epoch: 063, Train Loss: -0.26417375, Val loss: -0.2645, Test loss: -0.2645\n",
      "Epoch: 064, Train Loss: -0.26452291, Val loss: -0.2648, Test loss: -0.2648\n",
      "Epoch: 065, Train Loss: -0.26481473, Val loss: -0.2650, Test loss: -0.2650\n",
      "Epoch: 066, Train Loss: -0.26495159, Val loss: -0.2653, Test loss: -0.2653\n",
      "Epoch: 067, Train Loss: -0.26532662, Val loss: -0.2659, Test loss: -0.2659\n",
      "Epoch: 068, Train Loss: -0.26594901, Val loss: -0.2666, Test loss: -0.2666\n",
      "Epoch: 069, Train Loss: -0.26664639, Val loss: -0.2677, Test loss: -0.2677\n",
      "Epoch: 070, Train Loss: -0.26773214, Val loss: -0.2696, Test loss: -0.2696\n",
      "Epoch: 071, Train Loss: -0.26964736, Val loss: -0.2720, Test loss: -0.2720\n",
      "Epoch: 072, Train Loss: -0.27195990, Val loss: -0.2735, Test loss: -0.2735\n",
      "Epoch: 073, Train Loss: -0.27345097, Val loss: -0.2735, Test loss: -0.2735\n",
      "Epoch: 074, Train Loss: -0.27353954, Val loss: -0.2732, Test loss: -0.2732\n",
      "Epoch: 075, Train Loss: -0.27319515, Val loss: -0.2729, Test loss: -0.2729\n",
      "Epoch: 076, Train Loss: -0.27286184, Val loss: -0.2726, Test loss: -0.2726\n",
      "Epoch: 077, Train Loss: -0.27264369, Val loss: -0.2727, Test loss: -0.2727\n",
      "Epoch: 078, Train Loss: -0.27268183, Val loss: -0.2729, Test loss: -0.2729\n",
      "Epoch: 079, Train Loss: -0.27288532, Val loss: -0.2731, Test loss: -0.2731\n",
      "Epoch: 080, Train Loss: -0.27308643, Val loss: -0.2733, Test loss: -0.2733\n",
      "Epoch: 081, Train Loss: -0.27328026, Val loss: -0.2735, Test loss: -0.2735\n",
      "Epoch: 082, Train Loss: -0.27354896, Val loss: -0.2739, Test loss: -0.2739\n",
      "Epoch: 083, Train Loss: -0.27390802, Val loss: -0.2742, Test loss: -0.2742\n",
      "Epoch: 084, Train Loss: -0.27424192, Val loss: -0.2745, Test loss: -0.2745\n",
      "Epoch: 085, Train Loss: -0.27448034, Val loss: -0.2747, Test loss: -0.2747\n",
      "Epoch: 086, Train Loss: -0.27467549, Val loss: -0.2749, Test loss: -0.2749\n",
      "Epoch: 087, Train Loss: -0.27489209, Val loss: -0.2751, Test loss: -0.2751\n",
      "Epoch: 088, Train Loss: -0.27509916, Val loss: -0.2752, Test loss: -0.2752\n",
      "Epoch: 089, Train Loss: -0.27522790, Val loss: -0.2753, Test loss: -0.2753\n",
      "Epoch: 090, Train Loss: -0.27530134, Val loss: -0.2754, Test loss: -0.2754\n",
      "Epoch: 091, Train Loss: -0.27540410, Val loss: -0.2756, Test loss: -0.2756\n",
      "Epoch: 092, Train Loss: -0.27557015, Val loss: -0.2758, Test loss: -0.2758\n",
      "Epoch: 093, Train Loss: -0.27578306, Val loss: -0.2760, Test loss: -0.2760\n",
      "Epoch: 094, Train Loss: -0.27602315, Val loss: -0.2763, Test loss: -0.2763\n",
      "Epoch: 095, Train Loss: -0.27630329, Val loss: -0.2767, Test loss: -0.2767\n",
      "Epoch: 096, Train Loss: -0.27665114, Val loss: -0.2770, Test loss: -0.2770\n",
      "Epoch: 097, Train Loss: -0.27703917, Val loss: -0.2774, Test loss: -0.2774\n",
      "Epoch: 098, Train Loss: -0.27743816, Val loss: -0.2779, Test loss: -0.2779\n",
      "Epoch: 099, Train Loss: -0.27786016, Val loss: -0.2784, Test loss: -0.2784\n",
      "Epoch: 100, Train Loss: -0.27835512, Val loss: -0.2790, Test loss: -0.2790\n",
      "Epoch: 101, Train Loss: -0.27904058, Val loss: -0.2802, Test loss: -0.2802\n",
      "Epoch: 102, Train Loss: -0.28016186, Val loss: -0.2823, Test loss: -0.2823\n",
      "Epoch: 103, Train Loss: -0.28226995, Val loss: -0.2858, Test loss: -0.2858\n",
      "Epoch: 104, Train Loss: -0.28578973, Val loss: -0.2889, Test loss: -0.2889\n",
      "Epoch: 105, Train Loss: -0.28889084, Val loss: -0.2898, Test loss: -0.2898\n",
      "Epoch: 106, Train Loss: -0.28979766, Val loss: -0.2904, Test loss: -0.2904\n",
      "Epoch: 107, Train Loss: -0.29037976, Val loss: -0.2914, Test loss: -0.2914\n",
      "Epoch: 108, Train Loss: -0.29137278, Val loss: -0.2924, Test loss: -0.2924\n",
      "Epoch: 109, Train Loss: -0.29235029, Val loss: -0.2935, Test loss: -0.2935\n",
      "Epoch: 110, Train Loss: -0.29345477, Val loss: -0.2949, Test loss: -0.2949\n",
      "Epoch: 111, Train Loss: -0.29489350, Val loss: -0.2967, Test loss: -0.2967\n",
      "Epoch: 112, Train Loss: -0.29666138, Val loss: -0.2988, Test loss: -0.2988\n",
      "Epoch: 113, Train Loss: -0.29875505, Val loss: -0.3009, Test loss: -0.3009\n",
      "Epoch: 114, Train Loss: -0.30085957, Val loss: -0.3026, Test loss: -0.3026\n",
      "Epoch: 115, Train Loss: -0.30255914, Val loss: -0.3036, Test loss: -0.3036\n",
      "Epoch: 116, Train Loss: -0.30355573, Val loss: -0.3039, Test loss: -0.3039\n",
      "Epoch: 117, Train Loss: -0.30393791, Val loss: -0.3040, Test loss: -0.3040\n",
      "Epoch: 118, Train Loss: -0.30404258, Val loss: -0.3040, Test loss: -0.3040\n",
      "Epoch: 119, Train Loss: -0.30404067, Val loss: -0.3041, Test loss: -0.3041\n",
      "Epoch: 120, Train Loss: -0.30409551, Val loss: -0.3041, Test loss: -0.3041\n",
      "Epoch: 121, Train Loss: -0.30412602, Val loss: -0.3041, Test loss: -0.3041\n",
      "Epoch: 122, Train Loss: -0.30412495, Val loss: -0.3041, Test loss: -0.3041\n",
      "Epoch: 123, Train Loss: -0.30413902, Val loss: -0.3042, Test loss: -0.3042\n",
      "Epoch: 124, Train Loss: -0.30419242, Val loss: -0.3043, Test loss: -0.3043\n",
      "Epoch: 125, Train Loss: -0.30431211, Val loss: -0.3044, Test loss: -0.3044\n",
      "Epoch: 126, Train Loss: -0.30443549, Val loss: -0.3046, Test loss: -0.3046\n",
      "Epoch: 127, Train Loss: -0.30455065, Val loss: -0.3047, Test loss: -0.3047\n",
      "Epoch: 128, Train Loss: -0.30465162, Val loss: -0.3047, Test loss: -0.3047\n",
      "Epoch: 129, Train Loss: -0.30472898, Val loss: -0.3048, Test loss: -0.3048\n",
      "Epoch: 130, Train Loss: -0.30478048, Val loss: -0.3048, Test loss: -0.3048\n",
      "Epoch: 131, Train Loss: -0.30480015, Val loss: -0.3048, Test loss: -0.3048\n",
      "Epoch: 132, Train Loss: -0.30481768, Val loss: -0.3049, Test loss: -0.3049\n",
      "Epoch: 133, Train Loss: -0.30485129, Val loss: -0.3049, Test loss: -0.3049\n",
      "Epoch: 134, Train Loss: -0.30489385, Val loss: -0.3049, Test loss: -0.3049\n",
      "Epoch: 135, Train Loss: -0.30494571, Val loss: -0.3050, Test loss: -0.3050\n",
      "Epoch: 136, Train Loss: -0.30501032, Val loss: -0.3051, Test loss: -0.3051\n",
      "Epoch: 137, Train Loss: -0.30508757, Val loss: -0.3052, Test loss: -0.3052\n",
      "Epoch: 138, Train Loss: -0.30516970, Val loss: -0.3052, Test loss: -0.3052\n",
      "Epoch: 139, Train Loss: -0.30523813, Val loss: -0.3053, Test loss: -0.3053\n",
      "Epoch: 140, Train Loss: -0.30529606, Val loss: -0.3053, Test loss: -0.3053\n",
      "Epoch: 141, Train Loss: -0.30534291, Val loss: -0.3054, Test loss: -0.3054\n",
      "Epoch: 142, Train Loss: -0.30538523, Val loss: -0.3054, Test loss: -0.3054\n",
      "Epoch: 143, Train Loss: -0.30542529, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 144, Train Loss: -0.30546021, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 145, Train Loss: -0.30549145, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 146, Train Loss: -0.30551660, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 147, Train Loss: -0.30553710, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 148, Train Loss: -0.30554557, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 149, Train Loss: -0.30554354, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 150, Train Loss: -0.30553222, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 151, Train Loss: -0.30550802, Val loss: -0.3054, Test loss: -0.3054\n",
      "Epoch: 152, Train Loss: -0.30544841, Val loss: -0.3054, Test loss: -0.3054\n",
      "Epoch: 153, Train Loss: -0.30536842, Val loss: -0.3052, Test loss: -0.3052\n",
      "Epoch: 154, Train Loss: -0.30523622, Val loss: -0.3051, Test loss: -0.3051\n",
      "Epoch: 155, Train Loss: -0.30510426, Val loss: -0.3050, Test loss: -0.3050\n",
      "Epoch: 156, Train Loss: -0.30496132, Val loss: -0.3050, Test loss: -0.3050\n",
      "Epoch: 157, Train Loss: -0.30498505, Val loss: -0.3051, Test loss: -0.3051\n",
      "Epoch: 158, Train Loss: -0.30513155, Val loss: -0.3054, Test loss: -0.3054\n",
      "Epoch: 159, Train Loss: -0.30542123, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 160, Train Loss: -0.30562854, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 161, Train Loss: -0.30565500, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 162, Train Loss: -0.30553424, Val loss: -0.3054, Test loss: -0.3054\n",
      "Epoch: 163, Train Loss: -0.30538344, Val loss: -0.3053, Test loss: -0.3053\n",
      "Epoch: 164, Train Loss: -0.30534697, Val loss: -0.3054, Test loss: -0.3054\n",
      "Epoch: 165, Train Loss: -0.30542994, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 166, Train Loss: -0.30558717, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 167, Train Loss: -0.30568683, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 168, Train Loss: -0.30568039, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 169, Train Loss: -0.30560541, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 170, Train Loss: -0.30553484, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 171, Train Loss: -0.30553603, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 172, Train Loss: -0.30559361, Val loss: -0.3057, Test loss: -0.3057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173, Train Loss: -0.30567396, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 174, Train Loss: -0.30571806, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 175, Train Loss: -0.30571020, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 176, Train Loss: -0.30567145, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 177, Train Loss: -0.30563378, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 178, Train Loss: -0.30562782, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 179, Train Loss: -0.30564928, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 180, Train Loss: -0.30568862, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 181, Train Loss: -0.30572200, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 182, Train Loss: -0.30573690, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 183, Train Loss: -0.30573297, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 184, Train Loss: -0.30571616, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 185, Train Loss: -0.30569839, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 186, Train Loss: -0.30568612, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 187, Train Loss: -0.30568433, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 188, Train Loss: -0.30569041, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 189, Train Loss: -0.30570459, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 190, Train Loss: -0.30571866, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 191, Train Loss: -0.30573308, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 192, Train Loss: -0.30574346, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 193, Train Loss: -0.30575013, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 194, Train Loss: -0.30575395, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 195, Train Loss: -0.30575478, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 196, Train Loss: -0.30575371, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 197, Train Loss: -0.30575073, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 198, Train Loss: -0.30574620, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 199, Train Loss: -0.30573952, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 200, Train Loss: -0.30573034, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 201, Train Loss: -0.30571532, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 202, Train Loss: -0.30569339, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 203, Train Loss: -0.30565643, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 204, Train Loss: -0.30559981, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 205, Train Loss: -0.30550098, Val loss: -0.3054, Test loss: -0.3054\n",
      "Epoch: 206, Train Loss: -0.30536377, Val loss: -0.3051, Test loss: -0.3051\n",
      "Epoch: 207, Train Loss: -0.30513382, Val loss: -0.3049, Test loss: -0.3049\n",
      "Epoch: 208, Train Loss: -0.30490243, Val loss: -0.3046, Test loss: -0.3046\n",
      "Epoch: 209, Train Loss: -0.30461407, Val loss: -0.3046, Test loss: -0.3046\n",
      "Epoch: 210, Train Loss: -0.30463350, Val loss: -0.3048, Test loss: -0.3048\n",
      "Epoch: 211, Train Loss: -0.30483389, Val loss: -0.3053, Test loss: -0.3053\n",
      "Epoch: 212, Train Loss: -0.30534017, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 213, Train Loss: -0.30570376, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 214, Train Loss: -0.30574501, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 215, Train Loss: -0.30552101, Val loss: -0.3053, Test loss: -0.3053\n",
      "Epoch: 216, Train Loss: -0.30526185, Val loss: -0.3052, Test loss: -0.3052\n",
      "Epoch: 217, Train Loss: -0.30524826, Val loss: -0.3054, Test loss: -0.3054\n",
      "Epoch: 218, Train Loss: -0.30543506, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 219, Train Loss: -0.30569029, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 220, Train Loss: -0.30577135, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 221, Train Loss: -0.30566084, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 222, Train Loss: -0.30551553, Val loss: -0.3055, Test loss: -0.3055\n",
      "Epoch: 223, Train Loss: -0.30547297, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 224, Train Loss: -0.30558968, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 225, Train Loss: -0.30572712, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 226, Train Loss: -0.30577540, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 227, Train Loss: -0.30571723, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 228, Train Loss: -0.30563056, Val loss: -0.3056, Test loss: -0.3056\n",
      "Epoch: 229, Train Loss: -0.30561090, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 230, Train Loss: -0.30566204, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 231, Train Loss: -0.30574107, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 232, Train Loss: -0.30577862, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 233, Train Loss: -0.30575800, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 234, Train Loss: -0.30571175, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 235, Train Loss: -0.30568349, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 236, Train Loss: -0.30569863, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 237, Train Loss: -0.30573750, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 238, Train Loss: -0.30577230, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 239, Train Loss: -0.30578101, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 240, Train Loss: -0.30576563, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 241, Train Loss: -0.30574310, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 242, Train Loss: -0.30572975, Val loss: -0.3057, Test loss: -0.3057\n",
      "Epoch: 243, Train Loss: -0.30573559, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 244, Train Loss: -0.30575264, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 245, Train Loss: -0.30577171, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 246, Train Loss: -0.30578327, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 247, Train Loss: -0.30578339, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 248, Train Loss: -0.30577600, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 249, Train Loss: -0.30576575, Val loss: -0.3058, Test loss: -0.3058\n",
      "Epoch: 250, Train Loss: -0.30575871, Val loss: -0.3058, Test loss: -0.3058\n",
      "Best Val loss: -0.3058, Test loss: -0.3058\n",
      "The obtained data Chameleon has 2277 nodes, 72202 edges, 128 features, 6 labels, \n",
      "Epoch: 001, Train Loss: 0.35237771, Val loss: 0.3506, Test loss: 0.3506\n",
      "Epoch: 002, Train Loss: 0.35057557, Val loss: 0.3492, Test loss: 0.3492\n",
      "Epoch: 003, Train Loss: 0.34922856, Val loss: 0.3482, Test loss: 0.3482\n",
      "Epoch: 004, Train Loss: 0.34822536, Val loss: 0.3475, Test loss: 0.3475\n",
      "Epoch: 005, Train Loss: 0.34750032, Val loss: 0.3470, Test loss: 0.3470\n",
      "Epoch: 006, Train Loss: 0.34700346, Val loss: 0.3467, Test loss: 0.3467\n",
      "Epoch: 007, Train Loss: 0.34668887, Val loss: 0.3465, Test loss: 0.3465\n",
      "Epoch: 008, Train Loss: 0.34651023, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 009, Train Loss: 0.34642202, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 010, Train Loss: 0.34638202, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 011, Train Loss: 0.34636623, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 012, Train Loss: 0.34636122, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 013, Train Loss: 0.34635973, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 014, Train Loss: 0.34635925, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 015, Train Loss: 0.34635919, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 016, Train Loss: 0.34635925, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 017, Train Loss: 0.34635925, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 018, Train Loss: 0.34635925, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 019, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 020, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 021, Train Loss: 0.34635919, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 022, Train Loss: 0.34635919, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 023, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 024, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 025, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 026, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 027, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 028, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 029, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 030, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 031, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 032, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 033, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 034, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 035, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 036, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 037, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 038, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 039, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 040, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 041, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 042, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 043, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 044, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 045, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 046, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 047, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 048, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 049, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 050, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 051, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 052, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 053, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 054, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 055, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 056, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 057, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 058, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 059, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 060, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 061, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 062, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 063, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 064, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 065, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 066, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 067, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 068, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 069, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 070, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 071, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 072, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 073, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 074, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 075, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 076, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 077, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 078, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 079, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 080, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 081, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 082, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 083, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 084, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 085, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 086, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 087, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 088, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 089, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 090, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 091, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 092, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 093, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 094, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 095, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 096, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 097, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 098, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 099, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 100, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 101, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 102, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 103, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 104, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 105, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 106, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 107, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 108, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 109, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 110, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 111, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 112, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 113, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 114, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 115, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 116, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 117, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 118, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 119, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 120, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 121, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 122, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 123, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 124, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 125, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 126, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 127, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 128, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 129, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 130, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 131, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 132, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 133, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 134, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 135, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 136, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 137, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 138, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 139, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 140, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 141, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 142, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 143, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 144, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 145, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 146, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 147, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 148, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 149, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 151, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 152, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 153, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 154, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 155, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 156, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 157, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 158, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 159, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 160, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 161, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 162, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 163, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 164, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 165, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 166, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 167, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 168, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 169, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 170, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 171, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 172, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 173, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 174, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 175, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 176, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 177, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 178, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 179, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 180, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 181, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 182, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 183, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 184, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 185, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 186, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 187, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 188, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 189, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 190, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 191, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 192, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 193, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 194, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 195, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 196, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 197, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 198, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 199, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 200, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 201, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 202, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 203, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 204, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 205, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 206, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 207, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 208, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 209, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 210, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 211, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 212, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 213, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 214, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 215, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 216, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 217, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 218, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 219, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 220, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 221, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 222, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 223, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 224, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 225, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 226, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 227, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 228, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 229, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 230, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 231, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 232, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 233, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 234, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 235, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 236, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 237, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 238, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 239, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 240, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 241, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 242, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 243, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 244, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 245, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 246, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 247, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 248, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 249, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Epoch: 250, Train Loss: 0.34635931, Val loss: 0.3464, Test loss: 0.3464\n",
      "Best Val loss: 0.3464, Test loss: 0.3464\n",
      "The obtained data Squirrel has 5201 nodes, 434146 edges, 128 features, 5 labels, \n",
      "Epoch: 001, Train Loss: 0.37218595, Val loss: 0.3712, Test loss: 0.3712\n",
      "Epoch: 002, Train Loss: 0.37124884, Val loss: 0.3706, Test loss: 0.3706\n",
      "Epoch: 003, Train Loss: 0.37062913, Val loss: 0.3702, Test loss: 0.3702\n",
      "Epoch: 004, Train Loss: 0.37023723, Val loss: 0.3700, Test loss: 0.3700\n",
      "Epoch: 005, Train Loss: 0.37000483, Val loss: 0.3699, Test loss: 0.3699\n",
      "Epoch: 006, Train Loss: 0.36987364, Val loss: 0.3698, Test loss: 0.3698\n",
      "Epoch: 007, Train Loss: 0.36980343, Val loss: 0.3698, Test loss: 0.3698\n",
      "Epoch: 008, Train Loss: 0.36976910, Val loss: 0.3698, Test loss: 0.3698\n",
      "Epoch: 009, Train Loss: 0.36975414, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 010, Train Loss: 0.36974871, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 011, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 012, Train Loss: 0.36974651, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 013, Train Loss: 0.36974627, Val loss: 0.3697, Test loss: 0.3697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Train Loss: 0.36974663, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 015, Train Loss: 0.36974680, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 016, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 017, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 018, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 019, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 020, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 021, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 022, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 023, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 024, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 025, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 026, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 027, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 028, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 029, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 030, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 031, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 032, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 033, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 034, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 035, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 036, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 037, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 038, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 039, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 040, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 041, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 042, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 043, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 044, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 045, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 046, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 047, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 048, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 049, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 050, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 051, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 052, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 053, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 054, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 055, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 056, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 057, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 058, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 059, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 060, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 061, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 062, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 063, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 064, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 065, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 066, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 067, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 068, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 069, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 070, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 071, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 072, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 073, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 074, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 075, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 076, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 077, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 078, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 079, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 080, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 081, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 082, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 083, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 084, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 085, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 086, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 087, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 088, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 089, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 090, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 091, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 092, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 093, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 094, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 095, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 096, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 097, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 098, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 099, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 100, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 101, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 102, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 103, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 104, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 105, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 106, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 107, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 108, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 109, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 110, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 111, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 112, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 113, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 114, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 115, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 116, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 117, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 118, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 119, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 120, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 121, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 122, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 123, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 124, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 125, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 126, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 127, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 129, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 130, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 131, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 132, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 133, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 134, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 135, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 136, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 137, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 138, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 139, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 140, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 141, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 142, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 143, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 144, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 145, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 146, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 147, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 148, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 149, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 150, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 151, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 152, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 153, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 154, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 155, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 156, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 157, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 158, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 159, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 160, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 161, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 162, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 163, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 164, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 165, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 166, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 167, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 168, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 169, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 170, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 171, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 172, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 173, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 174, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 175, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 176, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 177, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 178, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 179, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 180, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 181, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 182, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 183, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 184, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 185, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 186, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 187, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 188, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 189, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 190, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 191, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 192, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 193, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 194, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 195, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 196, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 197, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 198, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 199, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 200, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 201, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 202, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 203, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 204, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 205, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 206, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 207, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 208, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 209, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 210, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 211, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 212, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 213, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 214, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 215, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 216, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 217, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 218, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 219, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 220, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 221, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 222, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 223, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 224, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 225, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 226, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 227, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 228, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 229, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 230, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 231, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 232, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 233, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 234, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 235, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 236, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 237, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 238, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 239, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 240, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 241, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 242, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 243, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 244, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 245, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 246, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 247, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 248, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 249, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Epoch: 250, Train Loss: 0.36974669, Val loss: 0.3697, Test loss: 0.3697\n",
      "Best Val loss: 0.3697, Test loss: 0.3697\n",
      "The obtained data Crocodile has 11631 nodes, 360040 edges, 128 features, 5 labels, \n",
      "Epoch: 001, Train Loss: 0.38702685, Val loss: 0.3856, Test loss: 0.3856\n",
      "Epoch: 002, Train Loss: 0.38564163, Val loss: 0.3850, Test loss: 0.3850\n",
      "Epoch: 003, Train Loss: 0.38500899, Val loss: 0.3847, Test loss: 0.3847\n",
      "Epoch: 004, Train Loss: 0.38474292, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 005, Train Loss: 0.38464171, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 006, Train Loss: 0.38460678, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 007, Train Loss: 0.38459718, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 008, Train Loss: 0.38459158, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 009, Train Loss: 0.38459206, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 010, Train Loss: 0.38459289, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 011, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 012, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 013, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 014, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 015, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 016, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 017, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 018, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 019, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 020, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 021, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 022, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 023, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 024, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 025, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 026, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 027, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 028, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 029, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 030, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 031, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 032, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 033, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 034, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 035, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 036, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 037, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 038, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 039, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 040, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 041, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 042, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 043, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 044, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 045, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 046, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 047, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 048, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 049, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 050, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 051, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 052, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 053, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 054, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 055, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 056, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 057, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 058, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 059, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 060, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 061, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 062, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 063, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 064, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 065, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 066, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 067, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 068, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 069, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 070, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 071, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 072, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 073, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 074, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 075, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 076, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 077, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 078, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 079, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 080, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 081, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 082, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 083, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 084, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 085, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 086, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 087, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 088, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 089, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 090, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 091, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 092, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 093, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 094, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 095, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 096, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 097, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 098, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 099, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 100, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 101, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 102, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 103, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 104, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 106, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 107, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 108, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 109, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 110, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 111, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 112, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 113, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 114, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 115, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 116, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 117, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 118, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 119, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 120, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 121, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 122, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 123, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 124, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 125, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 126, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 127, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 128, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 129, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 130, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 131, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 132, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 133, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 134, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 135, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 136, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 137, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 138, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 139, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 140, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 141, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 142, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 143, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 144, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 145, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 146, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 147, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 148, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 149, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 150, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 151, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 152, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 153, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 154, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 155, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 156, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 157, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 158, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 159, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 160, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 161, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 162, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 163, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 164, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 165, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 166, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 167, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 168, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 169, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 170, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 171, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 172, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 173, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 174, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 175, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 176, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 177, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 178, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 179, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 180, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 181, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 182, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 183, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 184, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 185, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 186, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 187, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 188, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 189, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 190, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 191, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 192, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 193, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 194, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 195, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 196, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 197, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 198, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 199, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 200, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 201, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 202, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 203, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 204, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 205, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 206, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 207, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 208, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 209, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 210, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 211, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 212, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 213, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 214, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 215, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 216, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 217, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 218, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 220, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 221, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 222, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 223, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 224, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 225, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 226, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 227, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 228, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 229, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 230, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 231, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 232, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 233, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 234, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 235, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 236, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 237, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 238, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 239, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 240, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 241, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 242, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 243, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 244, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 245, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 246, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 247, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 248, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 249, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 250, Train Loss: 0.38459265, Val loss: 0.3846, Test loss: 0.3846\n",
      "Best Val loss: 0.3846, Test loss: 0.3846\n",
      "The obtained data Actor has 7600 nodes, 30019 edges, 932 features, 5 labels, \n",
      "Epoch: 001, Train Loss: 0.37748647, Val loss: 0.3775, Test loss: 0.3775\n",
      "Epoch: 002, Train Loss: 0.37747008, Val loss: 0.3775, Test loss: 0.3775\n",
      "Epoch: 003, Train Loss: 0.37746096, Val loss: 0.3775, Test loss: 0.3775\n",
      "Epoch: 004, Train Loss: 0.37745273, Val loss: 0.3774, Test loss: 0.3774\n",
      "Epoch: 005, Train Loss: 0.37744355, Val loss: 0.3774, Test loss: 0.3774\n",
      "Epoch: 006, Train Loss: 0.37743163, Val loss: 0.3774, Test loss: 0.3774\n",
      "Epoch: 007, Train Loss: 0.37741435, Val loss: 0.3774, Test loss: 0.3774\n",
      "Epoch: 008, Train Loss: 0.37739003, Val loss: 0.3774, Test loss: 0.3774\n",
      "Epoch: 009, Train Loss: 0.37735534, Val loss: 0.3773, Test loss: 0.3773\n",
      "Epoch: 010, Train Loss: 0.37730730, Val loss: 0.3772, Test loss: 0.3772\n",
      "Epoch: 011, Train Loss: 0.37724006, Val loss: 0.3771, Test loss: 0.3771\n",
      "Epoch: 012, Train Loss: 0.37714756, Val loss: 0.3770, Test loss: 0.3770\n",
      "Epoch: 013, Train Loss: 0.37701929, Val loss: 0.3768, Test loss: 0.3768\n",
      "Epoch: 014, Train Loss: 0.37684405, Val loss: 0.3766, Test loss: 0.3766\n",
      "Epoch: 015, Train Loss: 0.37660444, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 016, Train Loss: 0.37627566, Val loss: 0.3758, Test loss: 0.3758\n",
      "Epoch: 017, Train Loss: 0.37582719, Val loss: 0.3752, Test loss: 0.3752\n",
      "Epoch: 018, Train Loss: 0.37521040, Val loss: 0.3744, Test loss: 0.3744\n",
      "Epoch: 019, Train Loss: 0.37436712, Val loss: 0.3732, Test loss: 0.3732\n",
      "Epoch: 020, Train Loss: 0.37319994, Val loss: 0.3716, Test loss: 0.3716\n",
      "Epoch: 021, Train Loss: 0.37159419, Val loss: 0.3694, Test loss: 0.3694\n",
      "Epoch: 022, Train Loss: 0.36938262, Val loss: 0.3664, Test loss: 0.3664\n",
      "Epoch: 023, Train Loss: 0.36636400, Val loss: 0.3623, Test loss: 0.3623\n",
      "Epoch: 024, Train Loss: 0.36229539, Val loss: 0.3569, Test loss: 0.3569\n",
      "Epoch: 025, Train Loss: 0.35693264, Val loss: 0.3502, Test loss: 0.3502\n",
      "Epoch: 026, Train Loss: 0.35015213, Val loss: 0.3421, Test loss: 0.3421\n",
      "Epoch: 027, Train Loss: 0.34208059, Val loss: 0.3333, Test loss: 0.3333\n",
      "Epoch: 028, Train Loss: 0.33328676, Val loss: 0.3245, Test loss: 0.3245\n",
      "Epoch: 029, Train Loss: 0.32450676, Val loss: 0.3158, Test loss: 0.3158\n",
      "Epoch: 030, Train Loss: 0.31583393, Val loss: 0.3064, Test loss: 0.3064\n",
      "Epoch: 031, Train Loss: 0.30636060, Val loss: 0.2955, Test loss: 0.2955\n",
      "Epoch: 032, Train Loss: 0.29545105, Val loss: 0.2834, Test loss: 0.2834\n",
      "Epoch: 033, Train Loss: 0.28339875, Val loss: 0.2706, Test loss: 0.2706\n",
      "Epoch: 034, Train Loss: 0.27063406, Val loss: 0.2565, Test loss: 0.2565\n",
      "Epoch: 035, Train Loss: 0.25648201, Val loss: 0.2408, Test loss: 0.2408\n",
      "Epoch: 036, Train Loss: 0.24084282, Val loss: 0.2245, Test loss: 0.2245\n",
      "Epoch: 037, Train Loss: 0.22450328, Val loss: 0.2059, Test loss: 0.2059\n",
      "Epoch: 038, Train Loss: 0.20586193, Val loss: 0.1843, Test loss: 0.1843\n",
      "Epoch: 039, Train Loss: 0.18427432, Val loss: 0.1606, Test loss: 0.1606\n",
      "Epoch: 040, Train Loss: 0.16062677, Val loss: 0.1325, Test loss: 0.1325\n",
      "Epoch: 041, Train Loss: 0.13253558, Val loss: 0.0988, Test loss: 0.0988\n",
      "Epoch: 042, Train Loss: 0.09881163, Val loss: 0.0562, Test loss: 0.0562\n",
      "Epoch: 043, Train Loss: 0.05619526, Val loss: -0.0091, Test loss: -0.0091\n",
      "Epoch: 044, Train Loss: -0.00912511, Val loss: -0.1150, Test loss: -0.1150\n",
      "Epoch: 045, Train Loss: -0.11501992, Val loss: -0.2329, Test loss: -0.2329\n",
      "Epoch: 046, Train Loss: -0.23293960, Val loss: -0.3275, Test loss: -0.3275\n",
      "Epoch: 047, Train Loss: -0.32754064, Val loss: -0.3831, Test loss: -0.3831\n",
      "Epoch: 048, Train Loss: -0.38305509, Val loss: -0.4329, Test loss: -0.4329\n",
      "Epoch: 049, Train Loss: -0.43291116, Val loss: -0.4740, Test loss: -0.4740\n",
      "Epoch: 050, Train Loss: -0.47398913, Val loss: -0.4945, Test loss: -0.4945\n",
      "Epoch: 051, Train Loss: -0.49453270, Val loss: -0.5136, Test loss: -0.5136\n",
      "Epoch: 052, Train Loss: -0.51364625, Val loss: -0.5258, Test loss: -0.5258\n",
      "Epoch: 053, Train Loss: -0.52580941, Val loss: -0.5314, Test loss: -0.5314\n",
      "Epoch: 054, Train Loss: -0.53141129, Val loss: -0.5491, Test loss: -0.5491\n",
      "Epoch: 055, Train Loss: -0.54908371, Val loss: -0.5600, Test loss: -0.5600\n",
      "Epoch: 056, Train Loss: -0.56003106, Val loss: -0.5767, Test loss: -0.5767\n",
      "Epoch: 057, Train Loss: -0.57667935, Val loss: -0.5938, Test loss: -0.5938\n",
      "Epoch: 058, Train Loss: -0.59375691, Val loss: -0.6065, Test loss: -0.6065\n",
      "Epoch: 059, Train Loss: -0.60648429, Val loss: -0.6284, Test loss: -0.6284\n",
      "Epoch: 060, Train Loss: -0.62836528, Val loss: -0.6470, Test loss: -0.6470\n",
      "Epoch: 061, Train Loss: -0.64704657, Val loss: -0.6683, Test loss: -0.6683\n",
      "Epoch: 062, Train Loss: -0.66833174, Val loss: -0.6937, Test loss: -0.6937\n",
      "Epoch: 063, Train Loss: -0.69373035, Val loss: -0.7211, Test loss: -0.7211\n",
      "Epoch: 064, Train Loss: -0.72111607, Val loss: -0.7544, Test loss: -0.7544\n",
      "Epoch: 065, Train Loss: -0.75444019, Val loss: -0.7853, Test loss: -0.7853\n",
      "Epoch: 066, Train Loss: -0.78534293, Val loss: -0.8187, Test loss: -0.8187\n",
      "Epoch: 067, Train Loss: -0.81868136, Val loss: -0.8480, Test loss: -0.8480\n",
      "Epoch: 068, Train Loss: -0.84800935, Val loss: -0.8752, Test loss: -0.8752\n",
      "Epoch: 069, Train Loss: -0.87515783, Val loss: -0.9003, Test loss: -0.9003\n",
      "Epoch: 070, Train Loss: -0.90034068, Val loss: -0.9203, Test loss: -0.9203\n",
      "Epoch: 071, Train Loss: -0.92026448, Val loss: -0.9386, Test loss: -0.9386\n",
      "Epoch: 072, Train Loss: -0.93864286, Val loss: -0.9538, Test loss: -0.9538\n",
      "Epoch: 073, Train Loss: -0.95375454, Val loss: -0.9667, Test loss: -0.9667\n",
      "Epoch: 074, Train Loss: -0.96665323, Val loss: -0.9788, Test loss: -0.9788\n",
      "Epoch: 075, Train Loss: -0.97875869, Val loss: -0.9884, Test loss: -0.9884\n",
      "Epoch: 076, Train Loss: -0.98842859, Val loss: -0.9973, Test loss: -0.9973\n",
      "Epoch: 077, Train Loss: -0.99727511, Val loss: -1.0055, Test loss: -1.0055\n",
      "Epoch: 078, Train Loss: -1.00553954, Val loss: -1.0121, Test loss: -1.0121\n",
      "Epoch: 079, Train Loss: -1.01205564, Val loss: -1.0183, Test loss: -1.0183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 080, Train Loss: -1.01829708, Val loss: -1.0242, Test loss: -1.0242\n",
      "Epoch: 081, Train Loss: -1.02417243, Val loss: -1.0287, Test loss: -1.0287\n",
      "Epoch: 082, Train Loss: -1.02869415, Val loss: -1.0327, Test loss: -1.0327\n",
      "Epoch: 083, Train Loss: -1.03274024, Val loss: -1.0368, Test loss: -1.0368\n",
      "Epoch: 084, Train Loss: -1.03678358, Val loss: -1.0402, Test loss: -1.0402\n",
      "Epoch: 085, Train Loss: -1.04017735, Val loss: -1.0430, Test loss: -1.0430\n",
      "Epoch: 086, Train Loss: -1.04301357, Val loss: -1.0459, Test loss: -1.0459\n",
      "Epoch: 087, Train Loss: -1.04588020, Val loss: -1.0486, Test loss: -1.0486\n",
      "Epoch: 088, Train Loss: -1.04855430, Val loss: -1.0508, Test loss: -1.0508\n",
      "Epoch: 089, Train Loss: -1.05075514, Val loss: -1.0528, Test loss: -1.0528\n",
      "Epoch: 090, Train Loss: -1.05277359, Val loss: -1.0548, Test loss: -1.0548\n",
      "Epoch: 091, Train Loss: -1.05480766, Val loss: -1.0566, Test loss: -1.0566\n",
      "Epoch: 092, Train Loss: -1.05662227, Val loss: -1.0581, Test loss: -1.0581\n",
      "Epoch: 093, Train Loss: -1.05808163, Val loss: -1.0594, Test loss: -1.0594\n",
      "Epoch: 094, Train Loss: -1.05940115, Val loss: -1.0607, Test loss: -1.0607\n",
      "Epoch: 095, Train Loss: -1.06074798, Val loss: -1.0621, Test loss: -1.0621\n",
      "Epoch: 096, Train Loss: -1.06205964, Val loss: -1.0632, Test loss: -1.0632\n",
      "Epoch: 097, Train Loss: -1.06322801, Val loss: -1.0642, Test loss: -1.0642\n",
      "Epoch: 098, Train Loss: -1.06424499, Val loss: -1.0652, Test loss: -1.0652\n",
      "Epoch: 099, Train Loss: -1.06519186, Val loss: -1.0661, Test loss: -1.0661\n",
      "Epoch: 100, Train Loss: -1.06611836, Val loss: -1.0670, Test loss: -1.0670\n",
      "Epoch: 101, Train Loss: -1.06696856, Val loss: -1.0677, Test loss: -1.0677\n",
      "Epoch: 102, Train Loss: -1.06770849, Val loss: -1.0684, Test loss: -1.0684\n",
      "Epoch: 103, Train Loss: -1.06838834, Val loss: -1.0691, Test loss: -1.0691\n",
      "Epoch: 104, Train Loss: -1.06905413, Val loss: -1.0697, Test loss: -1.0697\n",
      "Epoch: 105, Train Loss: -1.06971490, Val loss: -1.0704, Test loss: -1.0704\n",
      "Epoch: 106, Train Loss: -1.07035828, Val loss: -1.0710, Test loss: -1.0710\n",
      "Epoch: 107, Train Loss: -1.07095122, Val loss: -1.0715, Test loss: -1.0715\n",
      "Epoch: 108, Train Loss: -1.07148337, Val loss: -1.0720, Test loss: -1.0720\n",
      "Epoch: 109, Train Loss: -1.07197666, Val loss: -1.0724, Test loss: -1.0724\n",
      "Epoch: 110, Train Loss: -1.07244289, Val loss: -1.0729, Test loss: -1.0729\n",
      "Epoch: 111, Train Loss: -1.07288873, Val loss: -1.0733, Test loss: -1.0733\n",
      "Epoch: 112, Train Loss: -1.07332373, Val loss: -1.0737, Test loss: -1.0737\n",
      "Epoch: 113, Train Loss: -1.07374942, Val loss: -1.0742, Test loss: -1.0742\n",
      "Epoch: 114, Train Loss: -1.07416284, Val loss: -1.0746, Test loss: -1.0746\n",
      "Epoch: 115, Train Loss: -1.07456088, Val loss: -1.0749, Test loss: -1.0749\n",
      "Epoch: 116, Train Loss: -1.07493222, Val loss: -1.0753, Test loss: -1.0753\n",
      "Epoch: 117, Train Loss: -1.07527804, Val loss: -1.0756, Test loss: -1.0756\n",
      "Epoch: 118, Train Loss: -1.07560802, Val loss: -1.0759, Test loss: -1.0759\n",
      "Epoch: 119, Train Loss: -1.07591486, Val loss: -1.0762, Test loss: -1.0762\n",
      "Epoch: 120, Train Loss: -1.07620227, Val loss: -1.0765, Test loss: -1.0765\n",
      "Epoch: 121, Train Loss: -1.07646012, Val loss: -1.0767, Test loss: -1.0767\n",
      "Epoch: 122, Train Loss: -1.07666910, Val loss: -1.0768, Test loss: -1.0768\n",
      "Epoch: 123, Train Loss: -1.07680511, Val loss: -1.0768, Test loss: -1.0768\n",
      "Epoch: 124, Train Loss: -1.07682192, Val loss: -1.0767, Test loss: -1.0767\n",
      "Epoch: 125, Train Loss: -1.07667089, Val loss: -1.0764, Test loss: -1.0764\n",
      "Epoch: 126, Train Loss: -1.07639408, Val loss: -1.0762, Test loss: -1.0762\n",
      "Epoch: 127, Train Loss: -1.07622302, Val loss: -1.0767, Test loss: -1.0767\n",
      "Epoch: 128, Train Loss: -1.07666123, Val loss: -1.0777, Test loss: -1.0777\n",
      "Epoch: 129, Train Loss: -1.07767665, Val loss: -1.0786, Test loss: -1.0786\n",
      "Epoch: 130, Train Loss: -1.07859004, Val loss: -1.0788, Test loss: -1.0788\n",
      "Epoch: 131, Train Loss: -1.07876360, Val loss: -1.0784, Test loss: -1.0784\n",
      "Epoch: 132, Train Loss: -1.07841122, Val loss: -1.0782, Test loss: -1.0782\n",
      "Epoch: 133, Train Loss: -1.07824409, Val loss: -1.0787, Test loss: -1.0787\n",
      "Epoch: 134, Train Loss: -1.07866907, Val loss: -1.0794, Test loss: -1.0794\n",
      "Epoch: 135, Train Loss: -1.07937694, Val loss: -1.0797, Test loss: -1.0797\n",
      "Epoch: 136, Train Loss: -1.07971394, Val loss: -1.0796, Test loss: -1.0796\n",
      "Epoch: 137, Train Loss: -1.07958078, Val loss: -1.0794, Test loss: -1.0794\n",
      "Epoch: 138, Train Loss: -1.07943106, Val loss: -1.0796, Test loss: -1.0796\n",
      "Epoch: 139, Train Loss: -1.07963932, Val loss: -1.0801, Test loss: -1.0801\n",
      "Epoch: 140, Train Loss: -1.08011806, Val loss: -1.0805, Test loss: -1.0805\n",
      "Epoch: 141, Train Loss: -1.08045292, Val loss: -1.0805, Test loss: -1.0805\n",
      "Epoch: 142, Train Loss: -1.08046758, Val loss: -1.0804, Test loss: -1.0804\n",
      "Epoch: 143, Train Loss: -1.08037317, Val loss: -1.0805, Test loss: -1.0805\n",
      "Epoch: 144, Train Loss: -1.08045137, Val loss: -1.0807, Test loss: -1.0807\n",
      "Epoch: 145, Train Loss: -1.08074033, Val loss: -1.0810, Test loss: -1.0810\n",
      "Epoch: 146, Train Loss: -1.08104837, Val loss: -1.0812, Test loss: -1.0812\n",
      "Epoch: 147, Train Loss: -1.08118820, Val loss: -1.0812, Test loss: -1.0812\n",
      "Epoch: 148, Train Loss: -1.08117807, Val loss: -1.0812, Test loss: -1.0812\n",
      "Epoch: 149, Train Loss: -1.08117485, Val loss: -1.0813, Test loss: -1.0813\n",
      "Epoch: 150, Train Loss: -1.08129036, Val loss: -1.0815, Test loss: -1.0815\n",
      "Epoch: 151, Train Loss: -1.08150470, Val loss: -1.0817, Test loss: -1.0817\n",
      "Epoch: 152, Train Loss: -1.08171546, Val loss: -1.0818, Test loss: -1.0818\n",
      "Epoch: 153, Train Loss: -1.08183503, Val loss: -1.0819, Test loss: -1.0819\n",
      "Epoch: 154, Train Loss: -1.08187354, Val loss: -1.0819, Test loss: -1.0819\n",
      "Epoch: 155, Train Loss: -1.08189368, Val loss: -1.0820, Test loss: -1.0820\n",
      "Epoch: 156, Train Loss: -1.08195770, Val loss: -1.0821, Test loss: -1.0821\n",
      "Epoch: 157, Train Loss: -1.08208048, Val loss: -1.0822, Test loss: -1.0822\n",
      "Epoch: 158, Train Loss: -1.08223617, Val loss: -1.0824, Test loss: -1.0824\n",
      "Epoch: 159, Train Loss: -1.08237791, Val loss: -1.0825, Test loss: -1.0825\n",
      "Epoch: 160, Train Loss: -1.08248317, Val loss: -1.0826, Test loss: -1.0826\n",
      "Epoch: 161, Train Loss: -1.08255053, Val loss: -1.0826, Test loss: -1.0826\n",
      "Epoch: 162, Train Loss: -1.08259726, Val loss: -1.0826, Test loss: -1.0826\n",
      "Epoch: 163, Train Loss: -1.08264470, Val loss: -1.0827, Test loss: -1.0827\n",
      "Epoch: 164, Train Loss: -1.08270073, Val loss: -1.0828, Test loss: -1.0828\n",
      "Epoch: 165, Train Loss: -1.08277369, Val loss: -1.0829, Test loss: -1.0829\n",
      "Epoch: 166, Train Loss: -1.08285809, Val loss: -1.0830, Test loss: -1.0830\n",
      "Epoch: 167, Train Loss: -1.08295059, Val loss: -1.0830, Test loss: -1.0830\n",
      "Epoch: 168, Train Loss: -1.08304369, Val loss: -1.0831, Test loss: -1.0831\n",
      "Epoch: 169, Train Loss: -1.08313525, Val loss: -1.0832, Test loss: -1.0832\n",
      "Epoch: 170, Train Loss: -1.08322096, Val loss: -1.0833, Test loss: -1.0833\n",
      "Epoch: 171, Train Loss: -1.08330083, Val loss: -1.0834, Test loss: -1.0834\n",
      "Epoch: 172, Train Loss: -1.08337533, Val loss: -1.0834, Test loss: -1.0834\n",
      "Epoch: 173, Train Loss: -1.08344591, Val loss: -1.0835, Test loss: -1.0835\n",
      "Epoch: 174, Train Loss: -1.08351421, Val loss: -1.0836, Test loss: -1.0836\n",
      "Epoch: 175, Train Loss: -1.08357930, Val loss: -1.0836, Test loss: -1.0836\n",
      "Epoch: 176, Train Loss: -1.08364129, Val loss: -1.0837, Test loss: -1.0837\n",
      "Epoch: 177, Train Loss: -1.08370280, Val loss: -1.0838, Test loss: -1.0838\n",
      "Epoch: 178, Train Loss: -1.08376050, Val loss: -1.0838, Test loss: -1.0838\n",
      "Epoch: 179, Train Loss: -1.08381450, Val loss: -1.0839, Test loss: -1.0839\n",
      "Epoch: 180, Train Loss: -1.08385813, Val loss: -1.0839, Test loss: -1.0839\n",
      "Epoch: 181, Train Loss: -1.08388758, Val loss: -1.0839, Test loss: -1.0839\n",
      "Epoch: 182, Train Loss: -1.08388352, Val loss: -1.0838, Test loss: -1.0838\n",
      "Epoch: 183, Train Loss: -1.08380520, Val loss: -1.0836, Test loss: -1.0836\n",
      "Epoch: 184, Train Loss: -1.08356476, Val loss: -1.0830, Test loss: -1.0830\n",
      "Epoch: 185, Train Loss: -1.08300304, Val loss: -1.0819, Test loss: -1.0819\n",
      "Epoch: 186, Train Loss: -1.08188510, Val loss: -1.0802, Test loss: -1.0802\n",
      "Epoch: 187, Train Loss: -1.08020723, Val loss: -1.0791, Test loss: -1.0791\n",
      "Epoch: 188, Train Loss: -1.07909060, Val loss: -1.0803, Test loss: -1.0803\n",
      "Epoch: 189, Train Loss: -1.08033311, Val loss: -1.0834, Test loss: -1.0834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Train Loss: -1.08337414, Val loss: -1.0844, Test loss: -1.0844\n",
      "Epoch: 191, Train Loss: -1.08436799, Val loss: -1.0826, Test loss: -1.0826\n",
      "Epoch: 192, Train Loss: -1.08264875, Val loss: -1.0817, Test loss: -1.0817\n",
      "Epoch: 193, Train Loss: -1.08166683, Val loss: -1.0833, Test loss: -1.0833\n",
      "Epoch: 194, Train Loss: -1.08325648, Val loss: -1.0846, Test loss: -1.0846\n",
      "Epoch: 195, Train Loss: -1.08463752, Val loss: -1.0837, Test loss: -1.0837\n",
      "Epoch: 196, Train Loss: -1.08371937, Val loss: -1.0829, Test loss: -1.0829\n",
      "Epoch: 197, Train Loss: -1.08288920, Val loss: -1.0840, Test loss: -1.0840\n",
      "Epoch: 198, Train Loss: -1.08401501, Val loss: -1.0850, Test loss: -1.0850\n",
      "Epoch: 199, Train Loss: -1.08503449, Val loss: -1.0846, Test loss: -1.0846\n",
      "Epoch: 200, Train Loss: -1.08464813, Val loss: -1.0851, Test loss: -1.0851\n",
      "Epoch: 201, Train Loss: -1.08506882, Val loss: -1.0872, Test loss: -1.0872\n",
      "Epoch: 202, Train Loss: -1.08715606, Val loss: -1.0878, Test loss: -1.0878\n",
      "Epoch: 203, Train Loss: -1.08775866, Val loss: -1.0871, Test loss: -1.0871\n",
      "Epoch: 204, Train Loss: -1.08712184, Val loss: -1.0872, Test loss: -1.0872\n",
      "Epoch: 205, Train Loss: -1.08721137, Val loss: -1.0880, Test loss: -1.0880\n",
      "Epoch: 206, Train Loss: -1.08803213, Val loss: -1.0882, Test loss: -1.0882\n",
      "Epoch: 207, Train Loss: -1.08818293, Val loss: -1.0878, Test loss: -1.0878\n",
      "Epoch: 208, Train Loss: -1.08776081, Val loss: -1.0879, Test loss: -1.0879\n",
      "Epoch: 209, Train Loss: -1.08788419, Val loss: -1.0884, Test loss: -1.0884\n",
      "Epoch: 210, Train Loss: -1.08837461, Val loss: -1.0884, Test loss: -1.0884\n",
      "Epoch: 211, Train Loss: -1.08836651, Val loss: -1.0881, Test loss: -1.0881\n",
      "Epoch: 212, Train Loss: -1.08811092, Val loss: -1.0883, Test loss: -1.0883\n",
      "Epoch: 213, Train Loss: -1.08827484, Val loss: -1.0886, Test loss: -1.0886\n",
      "Epoch: 214, Train Loss: -1.08859122, Val loss: -1.0886, Test loss: -1.0886\n",
      "Epoch: 215, Train Loss: -1.08855486, Val loss: -1.0884, Test loss: -1.0884\n",
      "Epoch: 216, Train Loss: -1.08841097, Val loss: -1.0885, Test loss: -1.0885\n",
      "Epoch: 217, Train Loss: -1.08854747, Val loss: -1.0888, Test loss: -1.0888\n",
      "Epoch: 218, Train Loss: -1.08877945, Val loss: -1.0888, Test loss: -1.0888\n",
      "Epoch: 219, Train Loss: -1.08878648, Val loss: -1.0887, Test loss: -1.0887\n",
      "Epoch: 220, Train Loss: -1.08869600, Val loss: -1.0888, Test loss: -1.0888\n",
      "Epoch: 221, Train Loss: -1.08878136, Val loss: -1.0890, Test loss: -1.0890\n",
      "Epoch: 222, Train Loss: -1.08895195, Val loss: -1.0890, Test loss: -1.0890\n",
      "Epoch: 223, Train Loss: -1.08898735, Val loss: -1.0889, Test loss: -1.0889\n",
      "Epoch: 224, Train Loss: -1.08893037, Val loss: -1.0890, Test loss: -1.0890\n",
      "Epoch: 225, Train Loss: -1.08896649, Val loss: -1.0891, Test loss: -1.0891\n",
      "Epoch: 226, Train Loss: -1.08908486, Val loss: -1.0891, Test loss: -1.0891\n",
      "Epoch: 227, Train Loss: -1.08914411, Val loss: -1.0891, Test loss: -1.0891\n",
      "Epoch: 228, Train Loss: -1.08912385, Val loss: -1.0891, Test loss: -1.0891\n",
      "Epoch: 229, Train Loss: -1.08913088, Val loss: -1.0892, Test loss: -1.0892\n",
      "Epoch: 230, Train Loss: -1.08921039, Val loss: -1.0893, Test loss: -1.0893\n",
      "Epoch: 231, Train Loss: -1.08928704, Val loss: -1.0893, Test loss: -1.0893\n",
      "Epoch: 232, Train Loss: -1.08930337, Val loss: -1.0893, Test loss: -1.0893\n",
      "Epoch: 233, Train Loss: -1.08929729, Val loss: -1.0893, Test loss: -1.0893\n",
      "Epoch: 234, Train Loss: -1.08933282, Val loss: -1.0894, Test loss: -1.0894\n",
      "Epoch: 235, Train Loss: -1.08939695, Val loss: -1.0894, Test loss: -1.0894\n",
      "Epoch: 236, Train Loss: -1.08944249, Val loss: -1.0895, Test loss: -1.0895\n",
      "Epoch: 237, Train Loss: -1.08945203, Val loss: -1.0895, Test loss: -1.0895\n",
      "Epoch: 238, Train Loss: -1.08946073, Val loss: -1.0895, Test loss: -1.0895\n",
      "Epoch: 239, Train Loss: -1.08949125, Val loss: -1.0895, Test loss: -1.0895\n",
      "Epoch: 240, Train Loss: -1.08953798, Val loss: -1.0896, Test loss: -1.0896\n",
      "Epoch: 241, Train Loss: -1.08957684, Val loss: -1.0896, Test loss: -1.0896\n",
      "Epoch: 242, Train Loss: -1.08959568, Val loss: -1.0896, Test loss: -1.0896\n",
      "Epoch: 243, Train Loss: -1.08961022, Val loss: -1.0896, Test loss: -1.0896\n",
      "Epoch: 244, Train Loss: -1.08963764, Val loss: -1.0897, Test loss: -1.0897\n",
      "Epoch: 245, Train Loss: -1.08967328, Val loss: -1.0897, Test loss: -1.0897\n",
      "Epoch: 246, Train Loss: -1.08971059, Val loss: -1.0897, Test loss: -1.0897\n",
      "Epoch: 247, Train Loss: -1.08973742, Val loss: -1.0898, Test loss: -1.0898\n",
      "Epoch: 248, Train Loss: -1.08975494, Val loss: -1.0898, Test loss: -1.0898\n",
      "Epoch: 249, Train Loss: -1.08977211, Val loss: -1.0898, Test loss: -1.0898\n",
      "Epoch: 250, Train Loss: -1.08979571, Val loss: -1.0898, Test loss: -1.0898\n",
      "Best Val loss: -1.0898, Test loss: -1.0898\n",
      "The obtained data DeezerEurope has 28281 nodes, 185504 edges, 128 features, 2 labels, \n",
      "Epoch: 001, Train Loss: 0.39806169, Val loss: 0.3959, Test loss: 0.3959\n",
      "Epoch: 002, Train Loss: 0.39592069, Val loss: 0.3954, Test loss: 0.3954\n",
      "Epoch: 003, Train Loss: 0.39536780, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 004, Train Loss: 0.39529848, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 005, Train Loss: 0.39528668, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 006, Train Loss: 0.39528883, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 007, Train Loss: 0.39528883, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 008, Train Loss: 0.39528918, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 009, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 010, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 011, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 012, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 013, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 014, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 015, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 016, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 017, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 018, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 019, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 020, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 021, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 022, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 023, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 024, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 025, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 026, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 027, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 028, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 029, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 030, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 031, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 032, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 033, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 034, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 035, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 036, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 037, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 038, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 039, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 040, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 041, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 042, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 043, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 044, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 045, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 046, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 047, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 048, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 049, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 051, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 052, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 053, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 054, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 055, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 056, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 057, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 058, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 059, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 060, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 061, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 062, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 063, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 064, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 065, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 066, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 067, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 068, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 069, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 070, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 071, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 072, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 073, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 074, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 075, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 076, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 077, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 078, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 079, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 080, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 081, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 082, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 083, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 084, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 085, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 086, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 087, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 088, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 089, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 090, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 091, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 092, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 093, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 094, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 095, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 096, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 097, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 098, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 099, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 100, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 101, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 102, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 103, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 104, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 105, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 106, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 107, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 108, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 109, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 110, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 111, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 112, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 113, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 114, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 115, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 116, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 117, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 118, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 119, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 120, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 121, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 122, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 123, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 124, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 125, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 126, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 127, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 128, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 129, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 130, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 131, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 132, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 133, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 134, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 135, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 136, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 137, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 138, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 139, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 140, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 141, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 142, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 143, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 144, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 145, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 146, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 147, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 148, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 149, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 150, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 151, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 152, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 153, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 154, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 155, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 156, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 157, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 158, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 159, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 160, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 161, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 162, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 163, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 164, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 165, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 166, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 167, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 168, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 169, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 170, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 171, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 172, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 173, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 174, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 175, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 176, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 177, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 178, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 179, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 180, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 181, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 182, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 183, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 184, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 185, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 186, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 187, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 188, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 189, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 190, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 191, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 192, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 193, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 194, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 195, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 196, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 197, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 198, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 199, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 200, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 201, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 202, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 203, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 204, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 205, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 206, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 207, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 208, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 209, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 210, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 211, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 212, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 213, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 214, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 215, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 216, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 217, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 218, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 219, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 220, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 221, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 222, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 223, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 224, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 225, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 226, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 227, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 228, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 229, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 230, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 231, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 232, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 233, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 234, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 235, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 236, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 237, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 238, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 239, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 240, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 241, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 242, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 243, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 244, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 245, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 246, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 247, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 248, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 249, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Epoch: 250, Train Loss: 0.39528906, Val loss: 0.3953, Test loss: 0.3953\n",
      "Best Val loss: 0.3953, Test loss: 0.3953\n",
      "The obtained data Cora has 2708 nodes, 10556 edges, 1433 features, 7 labels, \n",
      "Epoch: 001, Train Loss: 0.35229635, Val loss: 0.3523, Test loss: 0.3523\n",
      "Epoch: 002, Train Loss: 0.35225523, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 003, Train Loss: 0.35223603, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 004, Train Loss: 0.35222745, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 005, Train Loss: 0.35222399, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 006, Train Loss: 0.35222256, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 007, Train Loss: 0.35222167, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 008, Train Loss: 0.35222137, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 009, Train Loss: 0.35222107, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 010, Train Loss: 0.35222107, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 011, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 012, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 013, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 014, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 015, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 016, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 017, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 018, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 019, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 020, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 021, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 022, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 023, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 024, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 025, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 026, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 027, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 028, Train Loss: 0.35222119, Val loss: 0.3522, Test loss: 0.3522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 029, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 030, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 031, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 032, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 033, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 034, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 035, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 036, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 037, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 038, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 039, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 040, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 041, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 042, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 043, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 044, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 045, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 046, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 047, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 048, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 049, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 050, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 051, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 052, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 053, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 054, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 055, Train Loss: 0.35222119, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 056, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 057, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 058, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 059, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 060, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 061, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 062, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 063, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 064, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 065, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 066, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 067, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 068, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 069, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 070, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 071, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 072, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 073, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 074, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 075, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 076, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 077, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 078, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 079, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 080, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 081, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 082, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 083, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 084, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 085, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 086, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 087, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 088, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 089, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 090, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 091, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 092, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 093, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 094, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 095, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 096, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 097, Train Loss: 0.35222119, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 098, Train Loss: 0.35222107, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 099, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 100, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 101, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 102, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 103, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 104, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 105, Train Loss: 0.35222107, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 106, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 107, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 108, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 109, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 110, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 111, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 112, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 113, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 114, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 115, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 116, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 117, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 118, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 119, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 120, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 121, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 122, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 123, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 124, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 125, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 126, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 127, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 128, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 129, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 130, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 131, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 132, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 133, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 134, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 135, Train Loss: 0.35222065, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 136, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 137, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 138, Train Loss: 0.35222107, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 139, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 140, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 141, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 142, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 143, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 145, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 146, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 147, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 148, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 149, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 150, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 151, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 152, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 153, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 154, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 155, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 156, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 157, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 158, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 159, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 160, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 161, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 162, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 163, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 164, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 165, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 166, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 167, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 168, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 169, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 170, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 171, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 172, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 173, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 174, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 175, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 176, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 177, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 178, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 179, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 180, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 181, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 182, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 183, Train Loss: 0.35222107, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 184, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 185, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 186, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 187, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 188, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 189, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 190, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 191, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 192, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 193, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 194, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 195, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 196, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 197, Train Loss: 0.35222113, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 198, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 199, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 200, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 201, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 202, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 203, Train Loss: 0.35222107, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 204, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 205, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 206, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 207, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 208, Train Loss: 0.35222101, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 209, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 210, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 211, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 212, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 213, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 214, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 215, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 216, Train Loss: 0.35222089, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 217, Train Loss: 0.35222065, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 218, Train Loss: 0.35222077, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 219, Train Loss: 0.35222065, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 220, Train Loss: 0.35222030, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 221, Train Loss: 0.35222059, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 222, Train Loss: 0.35222054, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 223, Train Loss: 0.35222036, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 224, Train Loss: 0.35221982, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 225, Train Loss: 0.35221970, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 226, Train Loss: 0.35221940, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 227, Train Loss: 0.35221863, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 228, Train Loss: 0.35221785, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 229, Train Loss: 0.35221654, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 230, Train Loss: 0.35221428, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 231, Train Loss: 0.35221034, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 232, Train Loss: 0.35220444, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 233, Train Loss: 0.35219306, Val loss: 0.3522, Test loss: 0.3522\n",
      "Epoch: 234, Train Loss: 0.35217476, Val loss: 0.3521, Test loss: 0.3521\n",
      "Epoch: 235, Train Loss: 0.35214150, Val loss: 0.3521, Test loss: 0.3521\n",
      "Epoch: 236, Train Loss: 0.35208225, Val loss: 0.3520, Test loss: 0.3520\n",
      "Epoch: 237, Train Loss: 0.35197699, Val loss: 0.3518, Test loss: 0.3518\n",
      "Epoch: 238, Train Loss: 0.35179251, Val loss: 0.3515, Test loss: 0.3515\n",
      "Epoch: 239, Train Loss: 0.35147506, Val loss: 0.3510, Test loss: 0.3510\n",
      "Epoch: 240, Train Loss: 0.35095084, Val loss: 0.3501, Test loss: 0.3501\n",
      "Epoch: 241, Train Loss: 0.35012132, Val loss: 0.3488, Test loss: 0.3488\n",
      "Epoch: 242, Train Loss: 0.34877890, Val loss: 0.3466, Test loss: 0.3466\n",
      "Epoch: 243, Train Loss: 0.34664041, Val loss: 0.3439, Test loss: 0.3439\n",
      "Epoch: 244, Train Loss: 0.34389514, Val loss: 0.3411, Test loss: 0.3411\n",
      "Epoch: 245, Train Loss: 0.34105140, Val loss: 0.3382, Test loss: 0.3382\n",
      "Epoch: 246, Train Loss: 0.33821756, Val loss: 0.3352, Test loss: 0.3352\n",
      "Epoch: 247, Train Loss: 0.33520555, Val loss: 0.3325, Test loss: 0.3325\n",
      "Epoch: 248, Train Loss: 0.33251143, Val loss: 0.3305, Test loss: 0.3305\n",
      "Epoch: 249, Train Loss: 0.33054817, Val loss: 0.3290, Test loss: 0.3290\n",
      "Epoch: 250, Train Loss: 0.32900333, Val loss: 0.3277, Test loss: 0.3277\n",
      "Best Val loss: 0.3277, Test loss: 0.3277\n",
      "The obtained data Pubmed has 19717 nodes, 88648 edges, 500 features, 3 labels, \n",
      "Epoch: 001, Train Loss: 0.39151353, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 002, Train Loss: 0.39151275, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 003, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 004, Train Loss: 0.39151293, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 005, Train Loss: 0.39151329, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 006, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 008, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 009, Train Loss: 0.39151287, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 010, Train Loss: 0.39151287, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 011, Train Loss: 0.39151287, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 012, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 013, Train Loss: 0.39151227, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 014, Train Loss: 0.39151287, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 015, Train Loss: 0.39151204, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 016, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 017, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 018, Train Loss: 0.39151287, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 019, Train Loss: 0.39151227, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 020, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 021, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 022, Train Loss: 0.39151216, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 023, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 024, Train Loss: 0.39151275, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 025, Train Loss: 0.39151275, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 026, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 027, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 028, Train Loss: 0.39151275, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 029, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 030, Train Loss: 0.39151311, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 031, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 032, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 033, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 034, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 035, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 036, Train Loss: 0.39151293, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 037, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 038, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 039, Train Loss: 0.39151305, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 040, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 041, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 042, Train Loss: 0.39151293, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 043, Train Loss: 0.39151204, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 044, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 045, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 046, Train Loss: 0.39151216, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 047, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 048, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 049, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 050, Train Loss: 0.39151293, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 051, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 052, Train Loss: 0.39151287, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 053, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 054, Train Loss: 0.39151311, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 055, Train Loss: 0.39151275, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 056, Train Loss: 0.39151204, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 057, Train Loss: 0.39151204, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 058, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 059, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 060, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 061, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 062, Train Loss: 0.39151275, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 063, Train Loss: 0.39151275, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 064, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 065, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 066, Train Loss: 0.39151227, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 067, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 068, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 069, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 070, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 071, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 072, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 073, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 074, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 075, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 076, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 077, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 078, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 079, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 080, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 081, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 082, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 083, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 084, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 085, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 086, Train Loss: 0.39151227, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 087, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 088, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 089, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 090, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 091, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 092, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 093, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 094, Train Loss: 0.39151227, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 095, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 096, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 097, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 098, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 099, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 100, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 101, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 102, Train Loss: 0.39151281, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 103, Train Loss: 0.39151275, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 104, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 105, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 106, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 107, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 108, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 109, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 110, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 111, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 112, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 113, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 114, Train Loss: 0.39151281, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 115, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 116, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 117, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 118, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 119, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 120, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 122, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 123, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 124, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 125, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 126, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 127, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 128, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 129, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 130, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 131, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 132, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 133, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 134, Train Loss: 0.39151216, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 135, Train Loss: 0.39151204, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 136, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 137, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 138, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 139, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 140, Train Loss: 0.39151299, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 141, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 142, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 143, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 144, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 145, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 146, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 147, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 148, Train Loss: 0.39151216, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 149, Train Loss: 0.39151287, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 150, Train Loss: 0.39151227, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 151, Train Loss: 0.39151275, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 152, Train Loss: 0.39151281, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 153, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 154, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 155, Train Loss: 0.39151281, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 156, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 157, Train Loss: 0.39151216, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 158, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 159, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 160, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 161, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 162, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 163, Train Loss: 0.39151227, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 164, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 165, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 166, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 167, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 168, Train Loss: 0.39151281, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 169, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 170, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 171, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 172, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 173, Train Loss: 0.39151281, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 174, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 175, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 176, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 177, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 178, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 179, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 180, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 181, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 182, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 183, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 184, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 185, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 186, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 187, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 188, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 189, Train Loss: 0.39151227, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 190, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 191, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 192, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 193, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 194, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 195, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 196, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 197, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 198, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 199, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 200, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 201, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 202, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 203, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 204, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 205, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 206, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 207, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 208, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 209, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 210, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 211, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 212, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 213, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 214, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 215, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 216, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 217, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 218, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 219, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 220, Train Loss: 0.39151245, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 221, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 222, Train Loss: 0.39151287, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 223, Train Loss: 0.39151216, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 224, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 225, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 226, Train Loss: 0.39151216, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 227, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 228, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 229, Train Loss: 0.39151216, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 230, Train Loss: 0.39151251, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 231, Train Loss: 0.39151263, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 232, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 233, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 234, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 235, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 236, Train Loss: 0.39151299, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 237, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 238, Train Loss: 0.39151257, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 239, Train Loss: 0.39151216, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 240, Train Loss: 0.39151281, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 241, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 242, Train Loss: 0.39151281, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 243, Train Loss: 0.39151227, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 244, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 245, Train Loss: 0.39151269, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 246, Train Loss: 0.39151227, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 247, Train Loss: 0.39151281, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 248, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 249, Train Loss: 0.39151287, Val loss: 0.3915, Test loss: 0.3915\n",
      "Epoch: 250, Train Loss: 0.39151239, Val loss: 0.3915, Test loss: 0.3915\n",
      "Best Val loss: 0.3915, Test loss: 0.3915\n",
      "The obtained data Citeseer has 3327 nodes, 9104 edges, 3703 features, 6 labels, \n",
      "Epoch: 001, Train Loss: 0.35844135, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 002, Train Loss: 0.35840422, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 003, Train Loss: 0.35839623, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 004, Train Loss: 0.35839468, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 005, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 006, Train Loss: 0.35839450, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 007, Train Loss: 0.35839415, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 008, Train Loss: 0.35839450, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 009, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 010, Train Loss: 0.35839450, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 011, Train Loss: 0.35839415, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 012, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 013, Train Loss: 0.35839432, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 014, Train Loss: 0.35839450, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 015, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 016, Train Loss: 0.35839450, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 017, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 018, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 019, Train Loss: 0.35839444, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 020, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 021, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 022, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 023, Train Loss: 0.35839444, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 024, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 025, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 026, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 027, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 028, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 029, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 030, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 031, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 032, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 033, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 034, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 035, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 036, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 037, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 038, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 039, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 040, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 041, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 042, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 043, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 044, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 045, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 046, Train Loss: 0.35839438, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 047, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 048, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 049, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 050, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 051, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 052, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 053, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 054, Train Loss: 0.35839415, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 055, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 056, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 057, Train Loss: 0.35839415, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 058, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 059, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 060, Train Loss: 0.35839427, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 061, Train Loss: 0.35839415, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 062, Train Loss: 0.35839415, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 063, Train Loss: 0.35839403, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 064, Train Loss: 0.35839403, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 065, Train Loss: 0.35839391, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 066, Train Loss: 0.35839391, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 067, Train Loss: 0.35839379, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 068, Train Loss: 0.35839379, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 069, Train Loss: 0.35839379, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 070, Train Loss: 0.35839367, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 071, Train Loss: 0.35839355, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 072, Train Loss: 0.35839331, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 073, Train Loss: 0.35839319, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 074, Train Loss: 0.35839307, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 075, Train Loss: 0.35839283, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 076, Train Loss: 0.35839260, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 077, Train Loss: 0.35839224, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 078, Train Loss: 0.35839200, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 079, Train Loss: 0.35839164, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 080, Train Loss: 0.35839105, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 081, Train Loss: 0.35839045, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 082, Train Loss: 0.35838991, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 083, Train Loss: 0.35838902, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 084, Train Loss: 0.35838783, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 085, Train Loss: 0.35838616, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 086, Train Loss: 0.35838473, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 087, Train Loss: 0.35838270, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 088, Train Loss: 0.35838020, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 089, Train Loss: 0.35837698, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 090, Train Loss: 0.35837340, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 091, Train Loss: 0.35836875, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 092, Train Loss: 0.35836303, Val loss: 0.3584, Test loss: 0.3584\n",
      "Epoch: 093, Train Loss: 0.35835624, Val loss: 0.3583, Test loss: 0.3583\n",
      "Epoch: 094, Train Loss: 0.35834789, Val loss: 0.3583, Test loss: 0.3583\n",
      "Epoch: 095, Train Loss: 0.35833788, Val loss: 0.3583, Test loss: 0.3583\n",
      "Epoch: 096, Train Loss: 0.35832596, Val loss: 0.3583, Test loss: 0.3583\n",
      "Epoch: 097, Train Loss: 0.35831189, Val loss: 0.3583, Test loss: 0.3583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 098, Train Loss: 0.35829550, Val loss: 0.3583, Test loss: 0.3583\n",
      "Epoch: 099, Train Loss: 0.35827625, Val loss: 0.3583, Test loss: 0.3583\n",
      "Epoch: 100, Train Loss: 0.35825515, Val loss: 0.3582, Test loss: 0.3582\n",
      "Epoch: 101, Train Loss: 0.35823178, Val loss: 0.3582, Test loss: 0.3582\n",
      "Epoch: 102, Train Loss: 0.35820687, Val loss: 0.3582, Test loss: 0.3582\n",
      "Epoch: 103, Train Loss: 0.35818076, Val loss: 0.3582, Test loss: 0.3582\n",
      "Epoch: 104, Train Loss: 0.35815442, Val loss: 0.3581, Test loss: 0.3581\n",
      "Epoch: 105, Train Loss: 0.35812831, Val loss: 0.3581, Test loss: 0.3581\n",
      "Epoch: 106, Train Loss: 0.35810256, Val loss: 0.3581, Test loss: 0.3581\n",
      "Epoch: 107, Train Loss: 0.35807645, Val loss: 0.3580, Test loss: 0.3580\n",
      "Epoch: 108, Train Loss: 0.35804856, Val loss: 0.3580, Test loss: 0.3580\n",
      "Epoch: 109, Train Loss: 0.35801768, Val loss: 0.3580, Test loss: 0.3580\n",
      "Epoch: 110, Train Loss: 0.35797977, Val loss: 0.3579, Test loss: 0.3579\n",
      "Epoch: 111, Train Loss: 0.35792983, Val loss: 0.3579, Test loss: 0.3579\n",
      "Epoch: 112, Train Loss: 0.35786372, Val loss: 0.3578, Test loss: 0.3578\n",
      "Epoch: 113, Train Loss: 0.35777557, Val loss: 0.3577, Test loss: 0.3577\n",
      "Epoch: 114, Train Loss: 0.35765845, Val loss: 0.3575, Test loss: 0.3575\n",
      "Epoch: 115, Train Loss: 0.35750532, Val loss: 0.3573, Test loss: 0.3573\n",
      "Epoch: 116, Train Loss: 0.35730690, Val loss: 0.3570, Test loss: 0.3570\n",
      "Epoch: 117, Train Loss: 0.35704714, Val loss: 0.3567, Test loss: 0.3567\n",
      "Epoch: 118, Train Loss: 0.35670000, Val loss: 0.3562, Test loss: 0.3562\n",
      "Epoch: 119, Train Loss: 0.35622877, Val loss: 0.3556, Test loss: 0.3556\n",
      "Epoch: 120, Train Loss: 0.35558766, Val loss: 0.3547, Test loss: 0.3547\n",
      "Epoch: 121, Train Loss: 0.35468912, Val loss: 0.3534, Test loss: 0.3534\n",
      "Epoch: 122, Train Loss: 0.35344148, Val loss: 0.3517, Test loss: 0.3517\n",
      "Epoch: 123, Train Loss: 0.35173917, Val loss: 0.3495, Test loss: 0.3495\n",
      "Epoch: 124, Train Loss: 0.34949815, Val loss: 0.3466, Test loss: 0.3466\n",
      "Epoch: 125, Train Loss: 0.34659046, Val loss: 0.3429, Test loss: 0.3429\n",
      "Epoch: 126, Train Loss: 0.34293467, Val loss: 0.3390, Test loss: 0.3390\n",
      "Epoch: 127, Train Loss: 0.33895820, Val loss: 0.3362, Test loss: 0.3362\n",
      "Epoch: 128, Train Loss: 0.33617377, Val loss: 0.3357, Test loss: 0.3357\n",
      "Epoch: 129, Train Loss: 0.33571917, Val loss: 0.3363, Test loss: 0.3363\n",
      "Epoch: 130, Train Loss: 0.33628863, Val loss: 0.3366, Test loss: 0.3366\n",
      "Epoch: 131, Train Loss: 0.33662009, Val loss: 0.3365, Test loss: 0.3365\n",
      "Epoch: 132, Train Loss: 0.33650124, Val loss: 0.3360, Test loss: 0.3360\n",
      "Epoch: 133, Train Loss: 0.33604068, Val loss: 0.3354, Test loss: 0.3354\n",
      "Epoch: 134, Train Loss: 0.33540630, Val loss: 0.3348, Test loss: 0.3348\n",
      "Epoch: 135, Train Loss: 0.33483475, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 136, Train Loss: 0.33459663, Val loss: 0.3347, Test loss: 0.3347\n",
      "Epoch: 137, Train Loss: 0.33474582, Val loss: 0.3350, Test loss: 0.3350\n",
      "Epoch: 138, Train Loss: 0.33503586, Val loss: 0.3352, Test loss: 0.3352\n",
      "Epoch: 139, Train Loss: 0.33519125, Val loss: 0.3351, Test loss: 0.3351\n",
      "Epoch: 140, Train Loss: 0.33514833, Val loss: 0.3350, Test loss: 0.3350\n",
      "Epoch: 141, Train Loss: 0.33497870, Val loss: 0.3348, Test loss: 0.3348\n",
      "Epoch: 142, Train Loss: 0.33478492, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 143, Train Loss: 0.33464527, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 144, Train Loss: 0.33459091, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 145, Train Loss: 0.33461010, Val loss: 0.3347, Test loss: 0.3347\n",
      "Epoch: 146, Train Loss: 0.33466679, Val loss: 0.3347, Test loss: 0.3347\n",
      "Epoch: 147, Train Loss: 0.33472294, Val loss: 0.3348, Test loss: 0.3348\n",
      "Epoch: 148, Train Loss: 0.33475351, Val loss: 0.3348, Test loss: 0.3348\n",
      "Epoch: 149, Train Loss: 0.33475018, Val loss: 0.3347, Test loss: 0.3347\n",
      "Epoch: 150, Train Loss: 0.33471757, Val loss: 0.3347, Test loss: 0.3347\n",
      "Epoch: 151, Train Loss: 0.33466929, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 152, Train Loss: 0.33462214, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 153, Train Loss: 0.33459151, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 154, Train Loss: 0.33458632, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 155, Train Loss: 0.33460391, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 156, Train Loss: 0.33462894, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 157, Train Loss: 0.33464283, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 158, Train Loss: 0.33463836, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 159, Train Loss: 0.33462030, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 160, Train Loss: 0.33459759, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 161, Train Loss: 0.33457893, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 162, Train Loss: 0.33456844, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 163, Train Loss: 0.33456618, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 164, Train Loss: 0.33456898, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 165, Train Loss: 0.33457220, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 166, Train Loss: 0.33457190, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 167, Train Loss: 0.33456618, Val loss: 0.3346, Test loss: 0.3346\n",
      "Epoch: 168, Train Loss: 0.33455473, Val loss: 0.3345, Test loss: 0.3345\n",
      "Epoch: 169, Train Loss: 0.33453822, Val loss: 0.3345, Test loss: 0.3345\n",
      "Epoch: 170, Train Loss: 0.33451629, Val loss: 0.3345, Test loss: 0.3345\n",
      "Epoch: 171, Train Loss: 0.33448756, Val loss: 0.3344, Test loss: 0.3344\n",
      "Epoch: 172, Train Loss: 0.33444399, Val loss: 0.3344, Test loss: 0.3344\n",
      "Epoch: 173, Train Loss: 0.33436686, Val loss: 0.3342, Test loss: 0.3342\n",
      "Epoch: 174, Train Loss: 0.33421749, Val loss: 0.3339, Test loss: 0.3339\n",
      "Epoch: 175, Train Loss: 0.33393055, Val loss: 0.3335, Test loss: 0.3335\n",
      "Epoch: 176, Train Loss: 0.33349240, Val loss: 0.3331, Test loss: 0.3331\n",
      "Epoch: 177, Train Loss: 0.33312529, Val loss: 0.3330, Test loss: 0.3330\n",
      "Epoch: 178, Train Loss: 0.33298761, Val loss: 0.3329, Test loss: 0.3329\n",
      "Epoch: 179, Train Loss: 0.33294433, Val loss: 0.3329, Test loss: 0.3329\n",
      "Epoch: 180, Train Loss: 0.33287317, Val loss: 0.3327, Test loss: 0.3327\n",
      "Epoch: 181, Train Loss: 0.33272964, Val loss: 0.3325, Test loss: 0.3325\n",
      "Epoch: 182, Train Loss: 0.33250803, Val loss: 0.3322, Test loss: 0.3322\n",
      "Epoch: 183, Train Loss: 0.33222818, Val loss: 0.3319, Test loss: 0.3319\n",
      "Epoch: 184, Train Loss: 0.33192539, Val loss: 0.3316, Test loss: 0.3316\n",
      "Epoch: 185, Train Loss: 0.33162624, Val loss: 0.3313, Test loss: 0.3313\n",
      "Epoch: 186, Train Loss: 0.33132720, Val loss: 0.3310, Test loss: 0.3310\n",
      "Epoch: 187, Train Loss: 0.33099407, Val loss: 0.3306, Test loss: 0.3306\n",
      "Epoch: 188, Train Loss: 0.33059090, Val loss: 0.3301, Test loss: 0.3301\n",
      "Epoch: 189, Train Loss: 0.33009422, Val loss: 0.3295, Test loss: 0.3295\n",
      "Epoch: 190, Train Loss: 0.32950300, Val loss: 0.3289, Test loss: 0.3289\n",
      "Epoch: 191, Train Loss: 0.32886308, Val loss: 0.3283, Test loss: 0.3283\n",
      "Epoch: 192, Train Loss: 0.32825780, Val loss: 0.3277, Test loss: 0.3277\n",
      "Epoch: 193, Train Loss: 0.32774830, Val loss: 0.3273, Test loss: 0.3273\n",
      "Epoch: 194, Train Loss: 0.32729363, Val loss: 0.3268, Test loss: 0.3268\n",
      "Epoch: 195, Train Loss: 0.32683635, Val loss: 0.3264, Test loss: 0.3264\n",
      "Epoch: 196, Train Loss: 0.32638443, Val loss: 0.3260, Test loss: 0.3260\n",
      "Epoch: 197, Train Loss: 0.32596517, Val loss: 0.3256, Test loss: 0.3256\n",
      "Epoch: 198, Train Loss: 0.32560492, Val loss: 0.3253, Test loss: 0.3253\n",
      "Epoch: 199, Train Loss: 0.32529783, Val loss: 0.3250, Test loss: 0.3250\n",
      "Epoch: 200, Train Loss: 0.32500732, Val loss: 0.3247, Test loss: 0.3247\n",
      "Epoch: 201, Train Loss: 0.32468891, Val loss: 0.3243, Test loss: 0.3243\n",
      "Epoch: 202, Train Loss: 0.32433212, Val loss: 0.3239, Test loss: 0.3239\n",
      "Epoch: 203, Train Loss: 0.32394409, Val loss: 0.3235, Test loss: 0.3235\n",
      "Epoch: 204, Train Loss: 0.32351863, Val loss: 0.3230, Test loss: 0.3230\n",
      "Epoch: 205, Train Loss: 0.32302368, Val loss: 0.3224, Test loss: 0.3224\n",
      "Epoch: 206, Train Loss: 0.32241762, Val loss: 0.3217, Test loss: 0.3217\n",
      "Epoch: 207, Train Loss: 0.32168353, Val loss: 0.3208, Test loss: 0.3208\n",
      "Epoch: 208, Train Loss: 0.32080871, Val loss: 0.3198, Test loss: 0.3198\n",
      "Epoch: 209, Train Loss: 0.31976271, Val loss: 0.3185, Test loss: 0.3185\n",
      "Epoch: 210, Train Loss: 0.31852853, Val loss: 0.3171, Test loss: 0.3171\n",
      "Epoch: 211, Train Loss: 0.31708252, Val loss: 0.3155, Test loss: 0.3155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212, Train Loss: 0.31548363, Val loss: 0.3137, Test loss: 0.3137\n",
      "Epoch: 213, Train Loss: 0.31371272, Val loss: 0.3117, Test loss: 0.3117\n",
      "Epoch: 214, Train Loss: 0.31174856, Val loss: 0.3097, Test loss: 0.3097\n",
      "Epoch: 215, Train Loss: 0.30974913, Val loss: 0.3081, Test loss: 0.3081\n",
      "Epoch: 216, Train Loss: 0.30813468, Val loss: 0.3071, Test loss: 0.3071\n",
      "Epoch: 217, Train Loss: 0.30712670, Val loss: 0.3066, Test loss: 0.3066\n",
      "Epoch: 218, Train Loss: 0.30658060, Val loss: 0.3063, Test loss: 0.3063\n",
      "Epoch: 219, Train Loss: 0.30631787, Val loss: 0.3062, Test loss: 0.3062\n",
      "Epoch: 220, Train Loss: 0.30619442, Val loss: 0.3061, Test loss: 0.3061\n",
      "Epoch: 221, Train Loss: 0.30611938, Val loss: 0.3061, Test loss: 0.3061\n",
      "Epoch: 222, Train Loss: 0.30606860, Val loss: 0.3060, Test loss: 0.3060\n",
      "Epoch: 223, Train Loss: 0.30601645, Val loss: 0.3060, Test loss: 0.3060\n",
      "Epoch: 224, Train Loss: 0.30595863, Val loss: 0.3059, Test loss: 0.3059\n",
      "Epoch: 225, Train Loss: 0.30590403, Val loss: 0.3059, Test loss: 0.3059\n",
      "Epoch: 226, Train Loss: 0.30585951, Val loss: 0.3058, Test loss: 0.3058\n",
      "Epoch: 227, Train Loss: 0.30582666, Val loss: 0.3058, Test loss: 0.3058\n",
      "Epoch: 228, Train Loss: 0.30579990, Val loss: 0.3058, Test loss: 0.3058\n",
      "Epoch: 229, Train Loss: 0.30576843, Val loss: 0.3057, Test loss: 0.3057\n",
      "Epoch: 230, Train Loss: 0.30572432, Val loss: 0.3057, Test loss: 0.3057\n",
      "Epoch: 231, Train Loss: 0.30566561, Val loss: 0.3056, Test loss: 0.3056\n",
      "Epoch: 232, Train Loss: 0.30559427, Val loss: 0.3055, Test loss: 0.3055\n",
      "Epoch: 233, Train Loss: 0.30551058, Val loss: 0.3054, Test loss: 0.3054\n",
      "Epoch: 234, Train Loss: 0.30541426, Val loss: 0.3053, Test loss: 0.3053\n",
      "Epoch: 235, Train Loss: 0.30530006, Val loss: 0.3052, Test loss: 0.3052\n",
      "Epoch: 236, Train Loss: 0.30515981, Val loss: 0.3050, Test loss: 0.3050\n",
      "Epoch: 237, Train Loss: 0.30499303, Val loss: 0.3048, Test loss: 0.3048\n",
      "Epoch: 238, Train Loss: 0.30482471, Val loss: 0.3047, Test loss: 0.3047\n",
      "Epoch: 239, Train Loss: 0.30473959, Val loss: 0.3047, Test loss: 0.3047\n",
      "Epoch: 240, Train Loss: 0.30474532, Val loss: 0.3046, Test loss: 0.3046\n",
      "Epoch: 241, Train Loss: 0.30455595, Val loss: 0.3043, Test loss: 0.3043\n",
      "Epoch: 242, Train Loss: 0.30432296, Val loss: 0.3042, Test loss: 0.3042\n",
      "Epoch: 243, Train Loss: 0.30421340, Val loss: 0.3041, Test loss: 0.3041\n",
      "Epoch: 244, Train Loss: 0.30412233, Val loss: 0.3040, Test loss: 0.3040\n",
      "Epoch: 245, Train Loss: 0.30395424, Val loss: 0.3037, Test loss: 0.3037\n",
      "Epoch: 246, Train Loss: 0.30370349, Val loss: 0.3034, Test loss: 0.3034\n",
      "Epoch: 247, Train Loss: 0.30343336, Val loss: 0.3032, Test loss: 0.3032\n",
      "Epoch: 248, Train Loss: 0.30320644, Val loss: 0.3030, Test loss: 0.3030\n",
      "Epoch: 249, Train Loss: 0.30296993, Val loss: 0.3026, Test loss: 0.3026\n",
      "Epoch: 250, Train Loss: 0.30259150, Val loss: 0.3021, Test loss: 0.3021\n",
      "Best Val loss: 0.3021, Test loss: 0.3021\n",
      "The obtained data Twitch-DE has 9498 nodes, 315774 edges, 128 features, 2 labels, \n",
      "Epoch: 001, Train Loss: 0.38377112, Val loss: 0.3825, Test loss: 0.3825\n",
      "Epoch: 002, Train Loss: 0.38249314, Val loss: 0.3818, Test loss: 0.3818\n",
      "Epoch: 003, Train Loss: 0.38183546, Val loss: 0.3815, Test loss: 0.3815\n",
      "Epoch: 004, Train Loss: 0.38154626, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 005, Train Loss: 0.38143814, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 006, Train Loss: 0.38140422, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 007, Train Loss: 0.38139373, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 008, Train Loss: 0.38139004, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 009, Train Loss: 0.38138884, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 010, Train Loss: 0.38138860, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 011, Train Loss: 0.38138860, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 012, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 013, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 014, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 015, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 016, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 017, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 018, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 019, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 020, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 021, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 022, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 023, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 024, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 025, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 026, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 027, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 028, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 029, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 030, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 031, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 032, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 033, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 034, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 035, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 036, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 037, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 038, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 039, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 040, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 041, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 042, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 043, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 044, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 045, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 046, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 047, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 048, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 049, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 050, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 051, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 052, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 053, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 054, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 055, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 056, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 057, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 058, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 059, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 060, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 061, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 062, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 063, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 064, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 065, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 066, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 067, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 068, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 069, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 070, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 071, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 072, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 073, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 074, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 075, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 076, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 077, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 078, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 079, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 080, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 081, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 082, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 083, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 084, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 085, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 086, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 087, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 088, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 089, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 090, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 091, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 092, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 093, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 094, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 095, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 096, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 097, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 098, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 099, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 100, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 101, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 102, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 103, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 104, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 105, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 106, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 107, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 108, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 109, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 110, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 111, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 112, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 113, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 114, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 115, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 116, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 117, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 118, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 119, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 120, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 121, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 122, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 123, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 124, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 125, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 126, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 127, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 128, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 129, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 130, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 131, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 132, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 133, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 134, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 135, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 136, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 137, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 138, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 139, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 140, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 141, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 142, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 143, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 144, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 145, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 146, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 147, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 148, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 149, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 150, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 151, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 152, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 153, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 154, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 155, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 156, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 157, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 158, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 159, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 160, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 161, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 162, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 163, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 164, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 165, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 166, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 167, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 168, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 169, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 170, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 171, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 172, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 173, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 174, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 175, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 176, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 177, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 178, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 179, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 180, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 181, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 182, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 183, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 184, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 185, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 186, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 187, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 188, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 189, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 190, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 191, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 192, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 193, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 194, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 195, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 196, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 197, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 198, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 199, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 200, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 201, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 202, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 203, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 204, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 205, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 206, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 207, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 208, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 209, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 210, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 211, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 212, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 213, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 214, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 215, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 216, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 217, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 218, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 219, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 220, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 221, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 222, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 223, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 224, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 225, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 226, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 227, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 228, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 229, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 230, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 231, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 232, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 233, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 234, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 235, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 236, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 237, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 238, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 239, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 240, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 241, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 242, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 243, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 244, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 245, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 246, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 247, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 248, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 249, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 250, Train Loss: 0.38138866, Val loss: 0.3814, Test loss: 0.3814\n",
      "Best Val loss: 0.3814, Test loss: 0.3814\n",
      "The obtained data Twitch-EN has 7126 nodes, 77774 edges, 128 features, 2 labels, \n",
      "Epoch: 001, Train Loss: 0.37953657, Val loss: 0.3781, Test loss: 0.3781\n",
      "Epoch: 002, Train Loss: 0.37809521, Val loss: 0.3773, Test loss: 0.3773\n",
      "Epoch: 003, Train Loss: 0.37725025, Val loss: 0.3768, Test loss: 0.3768\n",
      "Epoch: 004, Train Loss: 0.37677336, Val loss: 0.3765, Test loss: 0.3765\n",
      "Epoch: 005, Train Loss: 0.37651682, Val loss: 0.3764, Test loss: 0.3764\n",
      "Epoch: 006, Train Loss: 0.37638670, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 007, Train Loss: 0.37632447, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 008, Train Loss: 0.37629622, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 009, Train Loss: 0.37628520, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 010, Train Loss: 0.37628222, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 011, Train Loss: 0.37628078, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 012, Train Loss: 0.37628055, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 013, Train Loss: 0.37628067, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 014, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 015, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 016, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 017, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 018, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 019, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 020, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 021, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 022, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 023, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 024, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 025, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 026, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 027, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 028, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 029, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 030, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 031, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 032, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 033, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 034, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 035, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 036, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 037, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 038, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 039, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 040, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 041, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 042, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 043, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 044, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 045, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 046, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 047, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 048, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 049, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 050, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 051, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 052, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 053, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 054, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 055, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 056, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 057, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 058, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 059, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 060, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 061, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 062, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 063, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 064, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 065, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 066, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 067, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 068, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 069, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 070, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 071, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 072, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 073, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 074, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 075, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 076, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 077, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 078, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 079, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 080, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 081, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 082, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 083, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 084, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 085, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 086, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 087, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 088, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 089, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 090, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 091, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 092, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 093, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 094, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 095, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 096, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 097, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 098, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 099, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 100, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 101, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 102, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 103, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 104, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 105, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 106, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 107, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 108, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 109, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 110, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 111, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 112, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 113, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 114, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 115, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 116, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 117, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 118, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 119, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 120, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 121, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 122, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 123, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 124, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 125, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 126, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 127, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 128, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 129, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 130, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 131, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 132, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 133, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 134, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 135, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 136, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 137, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 138, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 139, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 140, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 141, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 142, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 143, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 144, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 145, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 146, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 147, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 148, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 149, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 150, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 151, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 152, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 153, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 154, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 155, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 156, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 157, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 158, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 159, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 160, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 161, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 162, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 163, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 164, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 165, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 167, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 168, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 169, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 170, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 171, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 172, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 173, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 174, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 175, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 176, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 177, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 178, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 179, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 180, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 181, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 182, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 183, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 184, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 185, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 186, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 187, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 188, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 189, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 190, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 191, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 192, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 193, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 194, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 195, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 196, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 197, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 198, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 199, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 200, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 201, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 202, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 203, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 204, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 205, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 206, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 207, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 208, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 209, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 210, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 211, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 212, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 213, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 214, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 215, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 216, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 217, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 218, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 219, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 220, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 221, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 222, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 223, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 224, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 225, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 226, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 227, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 228, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 229, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 230, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 231, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 232, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 233, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 234, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 235, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 236, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 237, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 238, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 239, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 240, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 241, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 242, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 243, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 244, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 245, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 246, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 247, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 248, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 249, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Epoch: 250, Train Loss: 0.37628126, Val loss: 0.3763, Test loss: 0.3763\n",
      "Best Val loss: 0.3763, Test loss: 0.3763\n",
      "The obtained data Twitch-ES has 4648 nodes, 123412 edges, 128 features, 2 labels, \n",
      "Epoch: 001, Train Loss: 0.37036991, Val loss: 0.3692, Test loss: 0.3692\n",
      "Epoch: 002, Train Loss: 0.36920696, Val loss: 0.3684, Test loss: 0.3684\n",
      "Epoch: 003, Train Loss: 0.36839640, Val loss: 0.3679, Test loss: 0.3679\n",
      "Epoch: 004, Train Loss: 0.36785096, Val loss: 0.3675, Test loss: 0.3675\n",
      "Epoch: 005, Train Loss: 0.36750448, Val loss: 0.3673, Test loss: 0.3673\n",
      "Epoch: 006, Train Loss: 0.36730075, Val loss: 0.3672, Test loss: 0.3672\n",
      "Epoch: 007, Train Loss: 0.36719155, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 008, Train Loss: 0.36713916, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 009, Train Loss: 0.36711705, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 010, Train Loss: 0.36710870, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 011, Train Loss: 0.36710578, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 012, Train Loss: 0.36710477, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 013, Train Loss: 0.36710513, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 014, Train Loss: 0.36710465, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 015, Train Loss: 0.36710465, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 016, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 017, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 018, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 019, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 020, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 021, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 022, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 023, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 024, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 025, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 026, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 027, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 028, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 029, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 030, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 031, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 032, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 033, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 034, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 035, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 036, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 037, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 038, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 039, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 040, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 041, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 042, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 043, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 044, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 045, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 046, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 047, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 048, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 049, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 050, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 051, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 052, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 053, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 054, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 055, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 056, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 057, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 058, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 059, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 060, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 061, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 062, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 063, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 064, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 065, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 066, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 067, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 068, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 069, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 070, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 071, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 072, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 073, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 074, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 075, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 076, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 077, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 078, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 079, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 080, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 081, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 082, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 083, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 084, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 085, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 086, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 087, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 088, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 089, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 090, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 091, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 092, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 093, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 094, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 095, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 096, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 097, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 098, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 099, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 100, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 101, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 102, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 103, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 104, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 105, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 106, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 107, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 108, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 109, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 110, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 111, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 112, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 113, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 114, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 115, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 116, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 117, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 118, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 119, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 120, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 121, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 122, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 123, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 124, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 125, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 126, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 127, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 128, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 129, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 130, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 131, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 132, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 133, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 134, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 135, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 136, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 137, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 138, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 139, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 140, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 141, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 142, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 143, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 144, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 145, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 146, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 147, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 148, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 149, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 150, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 151, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 152, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 153, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 154, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 155, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 156, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 157, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 158, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 159, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 160, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 161, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 162, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 163, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 164, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 165, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 166, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 167, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 168, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 169, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 170, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 171, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 172, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 173, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 174, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 175, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 176, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 177, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 178, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 179, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 180, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 181, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 182, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 183, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 184, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 185, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 186, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 187, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 188, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 189, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 190, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 191, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 192, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 193, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 194, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 195, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 196, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 197, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 198, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 199, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 200, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 201, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 202, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 203, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 204, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 205, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 206, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 207, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 208, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 209, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 210, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 211, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 212, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 213, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 214, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 215, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 216, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 217, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 218, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 219, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 220, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 221, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 222, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 223, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 224, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 225, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 226, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 227, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 228, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 229, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 230, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 231, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 232, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 233, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 234, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 235, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 236, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 237, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 238, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 239, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 240, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 241, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 242, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 243, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 244, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 245, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 246, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 247, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 248, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 249, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Epoch: 250, Train Loss: 0.36710489, Val loss: 0.3671, Test loss: 0.3671\n",
      "Best Val loss: 0.3671, Test loss: 0.3671\n",
      "The obtained data Twitch-FR has 6551 nodes, 231883 edges, 128 features, 2 labels, \n",
      "Epoch: 001, Train Loss: 0.37830049, Val loss: 0.3766, Test loss: 0.3766\n",
      "Epoch: 002, Train Loss: 0.37664562, Val loss: 0.3756, Test loss: 0.3756\n",
      "Epoch: 003, Train Loss: 0.37564760, Val loss: 0.3751, Test loss: 0.3751\n",
      "Epoch: 004, Train Loss: 0.37509722, Val loss: 0.3748, Test loss: 0.3748\n",
      "Epoch: 005, Train Loss: 0.37482047, Val loss: 0.3747, Test loss: 0.3747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Train Loss: 0.37469727, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 007, Train Loss: 0.37464976, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 008, Train Loss: 0.37463212, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 009, Train Loss: 0.37462384, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 010, Train Loss: 0.37461984, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 011, Train Loss: 0.37461740, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 012, Train Loss: 0.37461573, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 013, Train Loss: 0.37461591, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 014, Train Loss: 0.37461609, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 015, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 016, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 017, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 018, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 019, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 020, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 021, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 022, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 023, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 024, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 025, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 026, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 027, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 028, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 029, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 030, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 031, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 032, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 033, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 034, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 035, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 036, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 037, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 038, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 039, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 040, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 041, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 042, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 043, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 044, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 045, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 046, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 047, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 048, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 049, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 050, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 051, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 052, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 053, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 054, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 055, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 056, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 057, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 058, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 059, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 060, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 061, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 062, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 063, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 064, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 065, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 066, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 067, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 068, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 069, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 070, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 071, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 072, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 073, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 074, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 075, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 076, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 077, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 078, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 079, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 080, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 081, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 082, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 083, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 084, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 085, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 086, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 087, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 088, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 089, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 090, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 091, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 092, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 093, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 094, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 095, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 096, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 097, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 098, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 099, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 100, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 101, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 102, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 103, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 104, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 105, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 106, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 107, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 108, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 109, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 110, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 111, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 112, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 113, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 114, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 115, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 116, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 117, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 118, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 119, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 121, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 122, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 123, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 124, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 125, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 126, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 127, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 128, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 129, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 130, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 131, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 132, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 133, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 134, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 135, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 136, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 137, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 138, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 139, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 140, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 141, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 142, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 143, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 144, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 145, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 146, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 147, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 148, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 149, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 150, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 151, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 152, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 153, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 154, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 155, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 156, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 157, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 158, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 159, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 160, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 161, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 162, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 163, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 164, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 165, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 166, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 167, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 168, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 169, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 170, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 171, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 172, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 173, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 174, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 175, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 176, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 177, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 178, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 179, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 180, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 181, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 182, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 183, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 184, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 185, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 186, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 187, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 188, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 189, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 190, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 191, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 192, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 193, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 194, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 195, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 196, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 197, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 198, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 199, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 200, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 201, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 202, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 203, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 204, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 205, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 206, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 207, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 208, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 209, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 210, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 211, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 212, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 213, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 214, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 215, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 216, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 217, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 218, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 219, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 220, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 221, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 222, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 223, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 224, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 225, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 226, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 227, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 228, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 229, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 230, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 231, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 232, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 233, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 234, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 235, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 236, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 237, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 238, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 239, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 240, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 241, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 242, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 243, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 244, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 245, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 246, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 247, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 248, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 249, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Epoch: 250, Train Loss: 0.37461597, Val loss: 0.3746, Test loss: 0.3746\n",
      "Best Val loss: 0.3746, Test loss: 0.3746\n",
      "The obtained data Twitch-PT has 1912 nodes, 64510 edges, 128 features, 2 labels, \n",
      "Epoch: 001, Train Loss: 0.34582162, Val loss: 0.3441, Test loss: 0.3441\n",
      "Epoch: 002, Train Loss: 0.34414041, Val loss: 0.3429, Test loss: 0.3429\n",
      "Epoch: 003, Train Loss: 0.34288579, Val loss: 0.3420, Test loss: 0.3420\n",
      "Epoch: 004, Train Loss: 0.34196955, Val loss: 0.3413, Test loss: 0.3413\n",
      "Epoch: 005, Train Loss: 0.34132087, Val loss: 0.3409, Test loss: 0.3409\n",
      "Epoch: 006, Train Loss: 0.34087574, Val loss: 0.3406, Test loss: 0.3406\n",
      "Epoch: 007, Train Loss: 0.34057635, Val loss: 0.3404, Test loss: 0.3404\n",
      "Epoch: 008, Train Loss: 0.34038204, Val loss: 0.3403, Test loss: 0.3403\n",
      "Epoch: 009, Train Loss: 0.34026182, Val loss: 0.3402, Test loss: 0.3402\n",
      "Epoch: 010, Train Loss: 0.34019130, Val loss: 0.3402, Test loss: 0.3402\n",
      "Epoch: 011, Train Loss: 0.34015161, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 012, Train Loss: 0.34013003, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 013, Train Loss: 0.34011871, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 014, Train Loss: 0.34011191, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 015, Train Loss: 0.34010720, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 016, Train Loss: 0.34010458, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 017, Train Loss: 0.34010339, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 018, Train Loss: 0.34010291, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 019, Train Loss: 0.34010291, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 020, Train Loss: 0.34010291, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 021, Train Loss: 0.34010291, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 022, Train Loss: 0.34010297, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 023, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 024, Train Loss: 0.34010297, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 025, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 026, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 027, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 028, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 029, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 030, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 031, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 032, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 033, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 034, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 035, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 036, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 037, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 038, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 039, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 040, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 041, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 042, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 043, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 044, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 045, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 046, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 047, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 048, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 049, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 050, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 051, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 052, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 053, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 054, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 055, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 056, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 057, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 058, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 059, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 060, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 061, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 062, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 063, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 064, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 065, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 066, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 067, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 068, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 069, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 070, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 071, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 072, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 073, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 074, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 075, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 076, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 077, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 078, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 079, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 080, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 081, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 082, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 083, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 084, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 085, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 086, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 087, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 088, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 089, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 090, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 091, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 092, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 093, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 094, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 095, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 096, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 097, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 098, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 099, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 100, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 102, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 103, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 104, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 105, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 106, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 107, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 108, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 109, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 110, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 111, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 112, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 113, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 114, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 115, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 116, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 117, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 118, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 119, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 120, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 121, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 122, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 123, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 124, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 125, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 126, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 127, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 128, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 129, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 130, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 131, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 132, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 133, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 134, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 135, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 136, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 137, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 138, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 139, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 140, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 141, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 142, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 143, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 144, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 145, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 146, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 147, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 148, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 149, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 150, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 151, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 152, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 153, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 154, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 155, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 156, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 157, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 158, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 159, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 160, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 161, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 162, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 163, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 164, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 165, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 166, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 167, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 168, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 169, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 170, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 171, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 172, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 173, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 174, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 175, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 176, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 177, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 178, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 179, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 180, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 181, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 182, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 183, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 184, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 185, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 186, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 187, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 188, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 189, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 190, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 191, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 192, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 193, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 194, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 195, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 196, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 197, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 198, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 199, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 200, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 201, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 202, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 203, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 204, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 205, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 206, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 207, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 208, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 209, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 210, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 211, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 212, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 213, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 214, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 215, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 216, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 217, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 218, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 219, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 220, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 221, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 222, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 223, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 224, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 225, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 226, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 227, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 228, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 229, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 230, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 231, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 232, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 233, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 234, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 235, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 236, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 237, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 238, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 239, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 240, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 241, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 242, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 243, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 244, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 245, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 246, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 247, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 248, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 249, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Epoch: 250, Train Loss: 0.34010309, Val loss: 0.3401, Test loss: 0.3401\n",
      "Best Val loss: 0.3401, Test loss: 0.3401\n",
      "The obtained data Twitch-RU has 4385 nodes, 78993 edges, 128 features, 2 labels, \n",
      "Epoch: 001, Train Loss: 0.37100607, Val loss: 0.3689, Test loss: 0.3689\n",
      "Epoch: 002, Train Loss: 0.36894333, Val loss: 0.3676, Test loss: 0.3676\n",
      "Epoch: 003, Train Loss: 0.36761433, Val loss: 0.3668, Test loss: 0.3668\n",
      "Epoch: 004, Train Loss: 0.36679071, Val loss: 0.3663, Test loss: 0.3663\n",
      "Epoch: 005, Train Loss: 0.36629099, Val loss: 0.3660, Test loss: 0.3660\n",
      "Epoch: 006, Train Loss: 0.36600071, Val loss: 0.3658, Test loss: 0.3658\n",
      "Epoch: 007, Train Loss: 0.36584115, Val loss: 0.3658, Test loss: 0.3658\n",
      "Epoch: 008, Train Loss: 0.36576122, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 009, Train Loss: 0.36572665, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 010, Train Loss: 0.36571366, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 011, Train Loss: 0.36570966, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 012, Train Loss: 0.36570907, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 013, Train Loss: 0.36570871, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 014, Train Loss: 0.36570865, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 015, Train Loss: 0.36570865, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 016, Train Loss: 0.36570823, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 017, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 018, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 019, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 020, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 021, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 022, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 023, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 024, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 025, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 026, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 027, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 028, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 029, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 030, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 031, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 032, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 033, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 034, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 035, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 036, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 037, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 038, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 039, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 040, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 041, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 042, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 043, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 044, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 045, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 046, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 047, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 048, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 049, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 050, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 051, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 052, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 053, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 054, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 055, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 056, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 057, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 058, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 059, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 060, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 061, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 062, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 063, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 064, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 065, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 066, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 067, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 068, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 069, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 070, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 071, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 072, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 073, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 074, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 075, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 076, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 077, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 078, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 079, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 080, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 081, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 082, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 083, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 084, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 085, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 086, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 087, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 088, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 089, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 090, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 091, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 092, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 093, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 094, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 095, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 096, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 097, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 098, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 099, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 100, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 101, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 102, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 103, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 104, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 105, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 106, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 107, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 108, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 109, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 110, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 111, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 112, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 113, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 114, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 115, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 116, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 117, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 118, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 119, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 120, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 121, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 122, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 123, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 124, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 125, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 126, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 127, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 128, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 129, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 130, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 131, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 132, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 133, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 134, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 135, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 136, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 137, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 138, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 139, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 140, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 141, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 142, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 143, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 144, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 145, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 146, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 147, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 148, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 149, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 150, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 151, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 152, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 153, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 154, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 155, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 156, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 157, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 158, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 159, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 160, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 161, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 162, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 163, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 164, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 165, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 166, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 167, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 168, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 169, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 170, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 171, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 172, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 173, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 174, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 175, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 176, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 177, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 178, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 179, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 180, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 181, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 182, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 183, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 184, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 185, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 186, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 187, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 188, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 189, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 190, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 191, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 192, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 193, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 194, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 196, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 197, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 198, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 199, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 200, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 201, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 202, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 203, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 204, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 205, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 206, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 207, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 208, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 209, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 210, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 211, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 212, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 213, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 214, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 215, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 216, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 217, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 218, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 219, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 220, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 221, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 222, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 223, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 224, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 225, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 226, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 227, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 228, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 229, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 230, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 231, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 232, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 233, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 234, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 235, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 236, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 237, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 238, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 239, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 240, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 241, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 242, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 243, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 244, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 245, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 246, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 247, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 248, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 249, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Epoch: 250, Train Loss: 0.36570841, Val loss: 0.3657, Test loss: 0.3657\n",
      "Best Val loss: 0.3657, Test loss: 0.3657\n",
      "The obtained data WikiCS has 11701 nodes, 297110 edges, 300 features, 10 labels, \n",
      "Epoch: 001, Train Loss: 0.38470370, Val loss: 0.3847, Test loss: 0.3847\n",
      "Epoch: 002, Train Loss: 0.38469738, Val loss: 0.3847, Test loss: 0.3847\n",
      "Epoch: 003, Train Loss: 0.38469112, Val loss: 0.3847, Test loss: 0.3847\n",
      "Epoch: 004, Train Loss: 0.38468611, Val loss: 0.3847, Test loss: 0.3847\n",
      "Epoch: 005, Train Loss: 0.38467407, Val loss: 0.3847, Test loss: 0.3847\n",
      "Epoch: 006, Train Loss: 0.38465595, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 007, Train Loss: 0.38462520, Val loss: 0.3846, Test loss: 0.3846\n",
      "Epoch: 008, Train Loss: 0.38457274, Val loss: 0.3845, Test loss: 0.3845\n",
      "Epoch: 009, Train Loss: 0.38448870, Val loss: 0.3844, Test loss: 0.3844\n",
      "Epoch: 010, Train Loss: 0.38435054, Val loss: 0.3841, Test loss: 0.3841\n",
      "Epoch: 011, Train Loss: 0.38412905, Val loss: 0.3838, Test loss: 0.3838\n",
      "Epoch: 012, Train Loss: 0.38378370, Val loss: 0.3833, Test loss: 0.3833\n",
      "Epoch: 013, Train Loss: 0.38325286, Val loss: 0.3825, Test loss: 0.3825\n",
      "Epoch: 014, Train Loss: 0.38247561, Val loss: 0.3814, Test loss: 0.3814\n",
      "Epoch: 015, Train Loss: 0.38141906, Val loss: 0.3801, Test loss: 0.3801\n",
      "Epoch: 016, Train Loss: 0.38012099, Val loss: 0.3788, Test loss: 0.3788\n",
      "Epoch: 017, Train Loss: 0.37875187, Val loss: 0.3775, Test loss: 0.3775\n",
      "Epoch: 018, Train Loss: 0.37752545, Val loss: 0.3765, Test loss: 0.3765\n",
      "Epoch: 019, Train Loss: 0.37651265, Val loss: 0.3755, Test loss: 0.3755\n",
      "Epoch: 020, Train Loss: 0.37549353, Val loss: 0.3742, Test loss: 0.3742\n",
      "Epoch: 021, Train Loss: 0.37421143, Val loss: 0.3727, Test loss: 0.3727\n",
      "Epoch: 022, Train Loss: 0.37267566, Val loss: 0.3712, Test loss: 0.3712\n",
      "Epoch: 023, Train Loss: 0.37119174, Val loss: 0.3700, Test loss: 0.3700\n",
      "Epoch: 024, Train Loss: 0.37003469, Val loss: 0.3688, Test loss: 0.3688\n",
      "Epoch: 025, Train Loss: 0.36883187, Val loss: 0.3675, Test loss: 0.3675\n",
      "Epoch: 026, Train Loss: 0.36750340, Val loss: 0.3664, Test loss: 0.3664\n",
      "Epoch: 027, Train Loss: 0.36644757, Val loss: 0.3651, Test loss: 0.3651\n",
      "Epoch: 028, Train Loss: 0.36510980, Val loss: 0.3633, Test loss: 0.3633\n",
      "Epoch: 029, Train Loss: 0.36334348, Val loss: 0.3616, Test loss: 0.3616\n",
      "Epoch: 030, Train Loss: 0.36158538, Val loss: 0.3595, Test loss: 0.3595\n",
      "Epoch: 031, Train Loss: 0.35948169, Val loss: 0.3574, Test loss: 0.3574\n",
      "Epoch: 032, Train Loss: 0.35741138, Val loss: 0.3555, Test loss: 0.3555\n",
      "Epoch: 033, Train Loss: 0.35549521, Val loss: 0.3535, Test loss: 0.3535\n",
      "Epoch: 034, Train Loss: 0.35352361, Val loss: 0.3519, Test loss: 0.3519\n",
      "Epoch: 035, Train Loss: 0.35189378, Val loss: 0.3502, Test loss: 0.3502\n",
      "Epoch: 036, Train Loss: 0.35015333, Val loss: 0.3486, Test loss: 0.3486\n",
      "Epoch: 037, Train Loss: 0.34863961, Val loss: 0.3470, Test loss: 0.3470\n",
      "Epoch: 038, Train Loss: 0.34701121, Val loss: 0.3454, Test loss: 0.3454\n",
      "Epoch: 039, Train Loss: 0.34542096, Val loss: 0.3437, Test loss: 0.3437\n",
      "Epoch: 040, Train Loss: 0.34373391, Val loss: 0.3419, Test loss: 0.3419\n",
      "Epoch: 041, Train Loss: 0.34186208, Val loss: 0.3397, Test loss: 0.3397\n",
      "Epoch: 042, Train Loss: 0.33971155, Val loss: 0.3375, Test loss: 0.3375\n",
      "Epoch: 043, Train Loss: 0.33749294, Val loss: 0.3353, Test loss: 0.3353\n",
      "Epoch: 044, Train Loss: 0.33531237, Val loss: 0.3328, Test loss: 0.3328\n",
      "Epoch: 045, Train Loss: 0.33283353, Val loss: 0.3301, Test loss: 0.3301\n",
      "Epoch: 046, Train Loss: 0.33007193, Val loss: 0.3274, Test loss: 0.3274\n",
      "Epoch: 047, Train Loss: 0.32737136, Val loss: 0.3241, Test loss: 0.3241\n",
      "Epoch: 048, Train Loss: 0.32414770, Val loss: 0.3206, Test loss: 0.3206\n",
      "Epoch: 049, Train Loss: 0.32058871, Val loss: 0.3171, Test loss: 0.3171\n",
      "Epoch: 050, Train Loss: 0.31714690, Val loss: 0.3140, Test loss: 0.3140\n",
      "Epoch: 051, Train Loss: 0.31399393, Val loss: 0.3118, Test loss: 0.3118\n",
      "Epoch: 052, Train Loss: 0.31183326, Val loss: 0.3063, Test loss: 0.3063\n",
      "Epoch: 053, Train Loss: 0.30625665, Val loss: 0.3027, Test loss: 0.3027\n",
      "Epoch: 054, Train Loss: 0.30272102, Val loss: 0.3006, Test loss: 0.3006\n",
      "Epoch: 055, Train Loss: 0.30058336, Val loss: 0.2949, Test loss: 0.2949\n",
      "Epoch: 056, Train Loss: 0.29485989, Val loss: 0.2901, Test loss: 0.2901\n",
      "Epoch: 057, Train Loss: 0.29009199, Val loss: 0.2896, Test loss: 0.2896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 058, Train Loss: 0.28955257, Val loss: 0.2845, Test loss: 0.2845\n",
      "Epoch: 059, Train Loss: 0.28453362, Val loss: 0.2775, Test loss: 0.2775\n",
      "Epoch: 060, Train Loss: 0.27746630, Val loss: 0.2726, Test loss: 0.2726\n",
      "Epoch: 061, Train Loss: 0.27260852, Val loss: 0.2712, Test loss: 0.2712\n",
      "Epoch: 062, Train Loss: 0.27123058, Val loss: 0.2698, Test loss: 0.2698\n",
      "Epoch: 063, Train Loss: 0.26984751, Val loss: 0.2612, Test loss: 0.2612\n",
      "Epoch: 064, Train Loss: 0.26120365, Val loss: 0.2577, Test loss: 0.2577\n",
      "Epoch: 065, Train Loss: 0.25771070, Val loss: 0.2613, Test loss: 0.2613\n",
      "Epoch: 066, Train Loss: 0.26128483, Val loss: 0.2505, Test loss: 0.2505\n",
      "Epoch: 067, Train Loss: 0.25051785, Val loss: 0.2475, Test loss: 0.2475\n",
      "Epoch: 068, Train Loss: 0.24752879, Val loss: 0.2503, Test loss: 0.2503\n",
      "Epoch: 069, Train Loss: 0.25029421, Val loss: 0.2409, Test loss: 0.2409\n",
      "Epoch: 070, Train Loss: 0.24088848, Val loss: 0.2384, Test loss: 0.2384\n",
      "Epoch: 071, Train Loss: 0.23842692, Val loss: 0.2404, Test loss: 0.2404\n",
      "Epoch: 072, Train Loss: 0.24037325, Val loss: 0.2335, Test loss: 0.2335\n",
      "Epoch: 073, Train Loss: 0.23348510, Val loss: 0.2285, Test loss: 0.2285\n",
      "Epoch: 074, Train Loss: 0.22847557, Val loss: 0.2275, Test loss: 0.2275\n",
      "Epoch: 075, Train Loss: 0.22750270, Val loss: 0.2277, Test loss: 0.2277\n",
      "Epoch: 076, Train Loss: 0.22772956, Val loss: 0.2254, Test loss: 0.2254\n",
      "Epoch: 077, Train Loss: 0.22542286, Val loss: 0.2179, Test loss: 0.2179\n",
      "Epoch: 078, Train Loss: 0.21793723, Val loss: 0.2163, Test loss: 0.2163\n",
      "Epoch: 079, Train Loss: 0.21630490, Val loss: 0.2183, Test loss: 0.2183\n",
      "Epoch: 080, Train Loss: 0.21829093, Val loss: 0.2163, Test loss: 0.2163\n",
      "Epoch: 081, Train Loss: 0.21625113, Val loss: 0.2100, Test loss: 0.2100\n",
      "Epoch: 082, Train Loss: 0.21001661, Val loss: 0.2062, Test loss: 0.2062\n",
      "Epoch: 083, Train Loss: 0.20615602, Val loss: 0.2058, Test loss: 0.2058\n",
      "Epoch: 084, Train Loss: 0.20582640, Val loss: 0.2070, Test loss: 0.2070\n",
      "Epoch: 085, Train Loss: 0.20703065, Val loss: 0.2047, Test loss: 0.2047\n",
      "Epoch: 086, Train Loss: 0.20471001, Val loss: 0.1990, Test loss: 0.1990\n",
      "Epoch: 087, Train Loss: 0.19900966, Val loss: 0.1956, Test loss: 0.1956\n",
      "Epoch: 088, Train Loss: 0.19559455, Val loss: 0.1948, Test loss: 0.1948\n",
      "Epoch: 089, Train Loss: 0.19483531, Val loss: 0.1949, Test loss: 0.1949\n",
      "Epoch: 090, Train Loss: 0.19490421, Val loss: 0.1956, Test loss: 0.1956\n",
      "Epoch: 091, Train Loss: 0.19559884, Val loss: 0.1944, Test loss: 0.1944\n",
      "Epoch: 092, Train Loss: 0.19439507, Val loss: 0.1904, Test loss: 0.1904\n",
      "Epoch: 093, Train Loss: 0.19037330, Val loss: 0.1858, Test loss: 0.1858\n",
      "Epoch: 094, Train Loss: 0.18576026, Val loss: 0.1844, Test loss: 0.1844\n",
      "Epoch: 095, Train Loss: 0.18441212, Val loss: 0.1858, Test loss: 0.1858\n",
      "Epoch: 096, Train Loss: 0.18576777, Val loss: 0.1877, Test loss: 0.1877\n",
      "Epoch: 097, Train Loss: 0.18772447, Val loss: 0.1856, Test loss: 0.1856\n",
      "Epoch: 098, Train Loss: 0.18555236, Val loss: 0.1797, Test loss: 0.1797\n",
      "Epoch: 099, Train Loss: 0.17968905, Val loss: 0.1775, Test loss: 0.1775\n",
      "Epoch: 100, Train Loss: 0.17745447, Val loss: 0.1805, Test loss: 0.1805\n",
      "Epoch: 101, Train Loss: 0.18049908, Val loss: 0.1804, Test loss: 0.1804\n",
      "Epoch: 102, Train Loss: 0.18043399, Val loss: 0.1770, Test loss: 0.1770\n",
      "Epoch: 103, Train Loss: 0.17704797, Val loss: 0.1729, Test loss: 0.1729\n",
      "Epoch: 104, Train Loss: 0.17288494, Val loss: 0.1711, Test loss: 0.1711\n",
      "Epoch: 105, Train Loss: 0.17114246, Val loss: 0.1723, Test loss: 0.1723\n",
      "Epoch: 106, Train Loss: 0.17226815, Val loss: 0.1725, Test loss: 0.1725\n",
      "Epoch: 107, Train Loss: 0.17247272, Val loss: 0.1704, Test loss: 0.1704\n",
      "Epoch: 108, Train Loss: 0.17043042, Val loss: 0.1673, Test loss: 0.1673\n",
      "Epoch: 109, Train Loss: 0.16725528, Val loss: 0.1647, Test loss: 0.1647\n",
      "Epoch: 110, Train Loss: 0.16466784, Val loss: 0.1646, Test loss: 0.1646\n",
      "Epoch: 111, Train Loss: 0.16455770, Val loss: 0.1654, Test loss: 0.1654\n",
      "Epoch: 112, Train Loss: 0.16537893, Val loss: 0.1659, Test loss: 0.1659\n",
      "Epoch: 113, Train Loss: 0.16594708, Val loss: 0.1647, Test loss: 0.1647\n",
      "Epoch: 114, Train Loss: 0.16474557, Val loss: 0.1616, Test loss: 0.1616\n",
      "Epoch: 115, Train Loss: 0.16155457, Val loss: 0.1586, Test loss: 0.1586\n",
      "Epoch: 116, Train Loss: 0.15862918, Val loss: 0.1580, Test loss: 0.1580\n",
      "Epoch: 117, Train Loss: 0.15804660, Val loss: 0.1590, Test loss: 0.1590\n",
      "Epoch: 118, Train Loss: 0.15899086, Val loss: 0.1603, Test loss: 0.1603\n",
      "Epoch: 119, Train Loss: 0.16032541, Val loss: 0.1610, Test loss: 0.1610\n",
      "Epoch: 120, Train Loss: 0.16100931, Val loss: 0.1573, Test loss: 0.1573\n",
      "Epoch: 121, Train Loss: 0.15726590, Val loss: 0.1539, Test loss: 0.1539\n",
      "Epoch: 122, Train Loss: 0.15393782, Val loss: 0.1528, Test loss: 0.1528\n",
      "Epoch: 123, Train Loss: 0.15281641, Val loss: 0.1536, Test loss: 0.1536\n",
      "Epoch: 124, Train Loss: 0.15357161, Val loss: 0.1557, Test loss: 0.1557\n",
      "Epoch: 125, Train Loss: 0.15573227, Val loss: 0.1558, Test loss: 0.1558\n",
      "Epoch: 126, Train Loss: 0.15582418, Val loss: 0.1535, Test loss: 0.1535\n",
      "Epoch: 127, Train Loss: 0.15348291, Val loss: 0.1503, Test loss: 0.1503\n",
      "Epoch: 128, Train Loss: 0.15031385, Val loss: 0.1484, Test loss: 0.1484\n",
      "Epoch: 129, Train Loss: 0.14841592, Val loss: 0.1487, Test loss: 0.1487\n",
      "Epoch: 130, Train Loss: 0.14865935, Val loss: 0.1504, Test loss: 0.1504\n",
      "Epoch: 131, Train Loss: 0.15044391, Val loss: 0.1528, Test loss: 0.1528\n",
      "Epoch: 132, Train Loss: 0.15276468, Val loss: 0.1516, Test loss: 0.1516\n",
      "Epoch: 133, Train Loss: 0.15162265, Val loss: 0.1489, Test loss: 0.1489\n",
      "Epoch: 134, Train Loss: 0.14887643, Val loss: 0.1453, Test loss: 0.1453\n",
      "Epoch: 135, Train Loss: 0.14528692, Val loss: 0.1446, Test loss: 0.1446\n",
      "Epoch: 136, Train Loss: 0.14463830, Val loss: 0.1461, Test loss: 0.1461\n",
      "Epoch: 137, Train Loss: 0.14609265, Val loss: 0.1477, Test loss: 0.1477\n",
      "Epoch: 138, Train Loss: 0.14774740, Val loss: 0.1483, Test loss: 0.1483\n",
      "Epoch: 139, Train Loss: 0.14829826, Val loss: 0.1453, Test loss: 0.1453\n",
      "Epoch: 140, Train Loss: 0.14528394, Val loss: 0.1420, Test loss: 0.1420\n",
      "Epoch: 141, Train Loss: 0.14195669, Val loss: 0.1414, Test loss: 0.1414\n",
      "Epoch: 142, Train Loss: 0.14138198, Val loss: 0.1426, Test loss: 0.1426\n",
      "Epoch: 143, Train Loss: 0.14262581, Val loss: 0.1447, Test loss: 0.1447\n",
      "Epoch: 144, Train Loss: 0.14472723, Val loss: 0.1442, Test loss: 0.1442\n",
      "Epoch: 145, Train Loss: 0.14419341, Val loss: 0.1410, Test loss: 0.1410\n",
      "Epoch: 146, Train Loss: 0.14101613, Val loss: 0.1391, Test loss: 0.1391\n",
      "Epoch: 147, Train Loss: 0.13906515, Val loss: 0.1388, Test loss: 0.1388\n",
      "Epoch: 148, Train Loss: 0.13883436, Val loss: 0.1405, Test loss: 0.1405\n",
      "Epoch: 149, Train Loss: 0.14053881, Val loss: 0.1413, Test loss: 0.1413\n",
      "Epoch: 150, Train Loss: 0.14130664, Val loss: 0.1413, Test loss: 0.1413\n",
      "Epoch: 151, Train Loss: 0.14127362, Val loss: 0.1390, Test loss: 0.1390\n",
      "Epoch: 152, Train Loss: 0.13895929, Val loss: 0.1368, Test loss: 0.1368\n",
      "Epoch: 153, Train Loss: 0.13675714, Val loss: 0.1356, Test loss: 0.1356\n",
      "Epoch: 154, Train Loss: 0.13564718, Val loss: 0.1356, Test loss: 0.1356\n",
      "Epoch: 155, Train Loss: 0.13564765, Val loss: 0.1366, Test loss: 0.1366\n",
      "Epoch: 156, Train Loss: 0.13663924, Val loss: 0.1381, Test loss: 0.1381\n",
      "Epoch: 157, Train Loss: 0.13806105, Val loss: 0.1404, Test loss: 0.1404\n",
      "Epoch: 158, Train Loss: 0.14037812, Val loss: 0.1386, Test loss: 0.1386\n",
      "Epoch: 159, Train Loss: 0.13862538, Val loss: 0.1357, Test loss: 0.1357\n",
      "Epoch: 160, Train Loss: 0.13571203, Val loss: 0.1329, Test loss: 0.1329\n",
      "Epoch: 161, Train Loss: 0.13289416, Val loss: 0.1338, Test loss: 0.1338\n",
      "Epoch: 162, Train Loss: 0.13378930, Val loss: 0.1362, Test loss: 0.1362\n",
      "Epoch: 163, Train Loss: 0.13620734, Val loss: 0.1385, Test loss: 0.1385\n",
      "Epoch: 164, Train Loss: 0.13853014, Val loss: 0.1367, Test loss: 0.1367\n",
      "Epoch: 165, Train Loss: 0.13674510, Val loss: 0.1331, Test loss: 0.1331\n",
      "Epoch: 166, Train Loss: 0.13310957, Val loss: 0.1314, Test loss: 0.1314\n",
      "Epoch: 167, Train Loss: 0.13144374, Val loss: 0.1322, Test loss: 0.1322\n",
      "Epoch: 168, Train Loss: 0.13216782, Val loss: 0.1333, Test loss: 0.1333\n",
      "Epoch: 169, Train Loss: 0.13331950, Val loss: 0.1330, Test loss: 0.1330\n",
      "Epoch: 170, Train Loss: 0.13303506, Val loss: 0.1319, Test loss: 0.1319\n",
      "Epoch: 171, Train Loss: 0.13194728, Val loss: 0.1297, Test loss: 0.1297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172, Train Loss: 0.12971222, Val loss: 0.1290, Test loss: 0.1290\n",
      "Epoch: 173, Train Loss: 0.12901604, Val loss: 0.1288, Test loss: 0.1288\n",
      "Epoch: 174, Train Loss: 0.12877047, Val loss: 0.1295, Test loss: 0.1295\n",
      "Epoch: 175, Train Loss: 0.12954962, Val loss: 0.1298, Test loss: 0.1298\n",
      "Epoch: 176, Train Loss: 0.12980330, Val loss: 0.1308, Test loss: 0.1308\n",
      "Epoch: 177, Train Loss: 0.13078451, Val loss: 0.1308, Test loss: 0.1308\n",
      "Epoch: 178, Train Loss: 0.13082993, Val loss: 0.1300, Test loss: 0.1300\n",
      "Epoch: 179, Train Loss: 0.13002491, Val loss: 0.1283, Test loss: 0.1283\n",
      "Epoch: 180, Train Loss: 0.12826931, Val loss: 0.1268, Test loss: 0.1268\n",
      "Epoch: 181, Train Loss: 0.12682617, Val loss: 0.1259, Test loss: 0.1259\n",
      "Epoch: 182, Train Loss: 0.12589538, Val loss: 0.1257, Test loss: 0.1257\n",
      "Epoch: 183, Train Loss: 0.12567902, Val loss: 0.1262, Test loss: 0.1262\n",
      "Epoch: 184, Train Loss: 0.12622631, Val loss: 0.1275, Test loss: 0.1275\n",
      "Epoch: 185, Train Loss: 0.12749970, Val loss: 0.1295, Test loss: 0.1295\n",
      "Epoch: 186, Train Loss: 0.12947083, Val loss: 0.1305, Test loss: 0.1305\n",
      "Epoch: 187, Train Loss: 0.13046277, Val loss: 0.1303, Test loss: 0.1303\n",
      "Epoch: 188, Train Loss: 0.13032651, Val loss: 0.1256, Test loss: 0.1256\n",
      "Epoch: 189, Train Loss: 0.12558877, Val loss: 0.1242, Test loss: 0.1242\n",
      "Epoch: 190, Train Loss: 0.12416077, Val loss: 0.1257, Test loss: 0.1257\n",
      "Epoch: 191, Train Loss: 0.12569034, Val loss: 0.1286, Test loss: 0.1286\n",
      "Epoch: 192, Train Loss: 0.12860882, Val loss: 0.1295, Test loss: 0.1295\n",
      "Epoch: 193, Train Loss: 0.12949038, Val loss: 0.1256, Test loss: 0.1256\n",
      "Epoch: 194, Train Loss: 0.12560654, Val loss: 0.1231, Test loss: 0.1231\n",
      "Epoch: 195, Train Loss: 0.12307250, Val loss: 0.1238, Test loss: 0.1238\n",
      "Epoch: 196, Train Loss: 0.12384105, Val loss: 0.1255, Test loss: 0.1255\n",
      "Epoch: 197, Train Loss: 0.12545955, Val loss: 0.1258, Test loss: 0.1258\n",
      "Epoch: 198, Train Loss: 0.12581897, Val loss: 0.1241, Test loss: 0.1241\n",
      "Epoch: 199, Train Loss: 0.12409043, Val loss: 0.1214, Test loss: 0.1214\n",
      "Epoch: 200, Train Loss: 0.12140918, Val loss: 0.1219, Test loss: 0.1219\n",
      "Epoch: 201, Train Loss: 0.12189186, Val loss: 0.1224, Test loss: 0.1224\n",
      "Epoch: 202, Train Loss: 0.12242138, Val loss: 0.1232, Test loss: 0.1232\n",
      "Epoch: 203, Train Loss: 0.12321448, Val loss: 0.1220, Test loss: 0.1220\n",
      "Epoch: 204, Train Loss: 0.12199044, Val loss: 0.1206, Test loss: 0.1206\n",
      "Epoch: 205, Train Loss: 0.12059712, Val loss: 0.1204, Test loss: 0.1204\n",
      "Epoch: 206, Train Loss: 0.12036061, Val loss: 0.1203, Test loss: 0.1203\n",
      "Epoch: 207, Train Loss: 0.12027502, Val loss: 0.1213, Test loss: 0.1213\n",
      "Epoch: 208, Train Loss: 0.12125754, Val loss: 0.1217, Test loss: 0.1217\n",
      "Epoch: 209, Train Loss: 0.12171483, Val loss: 0.1217, Test loss: 0.1217\n",
      "Epoch: 210, Train Loss: 0.12172663, Val loss: 0.1212, Test loss: 0.1212\n",
      "Epoch: 211, Train Loss: 0.12124729, Val loss: 0.1207, Test loss: 0.1207\n",
      "Epoch: 212, Train Loss: 0.12068510, Val loss: 0.1205, Test loss: 0.1205\n",
      "Epoch: 213, Train Loss: 0.12050557, Val loss: 0.1197, Test loss: 0.1197\n",
      "Epoch: 214, Train Loss: 0.11971509, Val loss: 0.1196, Test loss: 0.1196\n",
      "Epoch: 215, Train Loss: 0.11959052, Val loss: 0.1189, Test loss: 0.1189\n",
      "Epoch: 216, Train Loss: 0.11893523, Val loss: 0.1189, Test loss: 0.1189\n",
      "Epoch: 217, Train Loss: 0.11885977, Val loss: 0.1185, Test loss: 0.1185\n",
      "Epoch: 218, Train Loss: 0.11850035, Val loss: 0.1181, Test loss: 0.1181\n",
      "Epoch: 219, Train Loss: 0.11813581, Val loss: 0.1179, Test loss: 0.1179\n",
      "Epoch: 220, Train Loss: 0.11793792, Val loss: 0.1178, Test loss: 0.1178\n",
      "Epoch: 221, Train Loss: 0.11778069, Val loss: 0.1183, Test loss: 0.1183\n",
      "Epoch: 222, Train Loss: 0.11833346, Val loss: 0.1190, Test loss: 0.1190\n",
      "Epoch: 223, Train Loss: 0.11895335, Val loss: 0.1206, Test loss: 0.1206\n",
      "Epoch: 224, Train Loss: 0.12057924, Val loss: 0.1218, Test loss: 0.1218\n",
      "Epoch: 225, Train Loss: 0.12179065, Val loss: 0.1231, Test loss: 0.1231\n",
      "Epoch: 226, Train Loss: 0.12306058, Val loss: 0.1203, Test loss: 0.1203\n",
      "Epoch: 227, Train Loss: 0.12032282, Val loss: 0.1176, Test loss: 0.1176\n",
      "Epoch: 228, Train Loss: 0.11759877, Val loss: 0.1172, Test loss: 0.1172\n",
      "Epoch: 229, Train Loss: 0.11720920, Val loss: 0.1178, Test loss: 0.1178\n",
      "Epoch: 230, Train Loss: 0.11778378, Val loss: 0.1183, Test loss: 0.1183\n",
      "Epoch: 231, Train Loss: 0.11828864, Val loss: 0.1198, Test loss: 0.1198\n",
      "Epoch: 232, Train Loss: 0.11975324, Val loss: 0.1174, Test loss: 0.1174\n",
      "Epoch: 233, Train Loss: 0.11735034, Val loss: 0.1180, Test loss: 0.1180\n",
      "Epoch: 234, Train Loss: 0.11801851, Val loss: 0.1161, Test loss: 0.1161\n",
      "Epoch: 235, Train Loss: 0.11606967, Val loss: 0.1156, Test loss: 0.1156\n",
      "Epoch: 236, Train Loss: 0.11558330, Val loss: 0.1156, Test loss: 0.1156\n",
      "Epoch: 237, Train Loss: 0.11560583, Val loss: 0.1154, Test loss: 0.1154\n",
      "Epoch: 238, Train Loss: 0.11541915, Val loss: 0.1167, Test loss: 0.1167\n",
      "Epoch: 239, Train Loss: 0.11667550, Val loss: 0.1183, Test loss: 0.1183\n",
      "Epoch: 240, Train Loss: 0.11830926, Val loss: 0.1194, Test loss: 0.1194\n",
      "Epoch: 241, Train Loss: 0.11935139, Val loss: 0.1178, Test loss: 0.1178\n",
      "Epoch: 242, Train Loss: 0.11784232, Val loss: 0.1148, Test loss: 0.1148\n",
      "Epoch: 243, Train Loss: 0.11479270, Val loss: 0.1141, Test loss: 0.1141\n",
      "Epoch: 244, Train Loss: 0.11414838, Val loss: 0.1162, Test loss: 0.1162\n",
      "Epoch: 245, Train Loss: 0.11621845, Val loss: 0.1187, Test loss: 0.1187\n",
      "Epoch: 246, Train Loss: 0.11869216, Val loss: 0.1197, Test loss: 0.1197\n",
      "Epoch: 247, Train Loss: 0.11969340, Val loss: 0.1165, Test loss: 0.1165\n",
      "Epoch: 248, Train Loss: 0.11645925, Val loss: 0.1137, Test loss: 0.1137\n",
      "Epoch: 249, Train Loss: 0.11366606, Val loss: 0.1147, Test loss: 0.1147\n",
      "Epoch: 250, Train Loss: 0.11465716, Val loss: 0.1167, Test loss: 0.1167\n",
      "Best Val loss: 0.1137, Test loss: 0.1137\n"
     ]
    }
   ],
   "source": [
    "pooling = 'mincut' #(options: diffpool, mincut)\n",
    "# dataset_name = 'Chameleon'\n",
    "\n",
    "\n",
    "\n",
    "#Pooling selector\n",
    "pooling_selector = {\n",
    "    'diffpool': dense_diff_pool,\n",
    "    'mincut': dense_mincut_pool\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 'WikiCS',\n",
    "# for dataset_name in ['Cornell', 'Texas', 'Wisconsin', \"Chameleon\", \"Squirrel\", \"Crocodile\", 'Actor',\n",
    "#                     'DeezerEurope', 'Cora', 'Pubmed', 'Citeseer', \n",
    "#                     'Twitch-DE', 'Twitch-EN', 'Twitch-ES', 'Twitch-FR', 'Twitch-PT', 'Twitch-RU', 'WikiCS']:\n",
    "\n",
    "\n",
    "for dataset_name in ['Cornell', 'Texas', 'Wisconsin', \"Chameleon\", \"Squirrel\", \"Crocodile\", 'Actor',\n",
    "                    'DeezerEurope', 'Cora', 'Pubmed', 'Citeseer', \n",
    "                    'Twitch-DE', 'Twitch-EN', 'Twitch-ES', 'Twitch-FR', 'Twitch-PT', 'Twitch-RU', 'WikiCS']:\n",
    "\n",
    "    #Creates outdir\n",
    "    outdir = 'results_node_clustering_5_perc/{}_{}'.format(dataset_name, pooling)\n",
    "    os.makedirs(outdir, exist_ok = True)\n",
    "\n",
    "    #Loads dataset\n",
    "    data = load_pyg_dataset(\n",
    "        data_name=dataset_name,\n",
    "        device=device\n",
    "    )\n",
    "#     num_clusters = data.y.max().tolist()+1\n",
    "    num_clusters = math.ceil(0.05 * data.num_nodes)\n",
    "    data.adj = to_dense_adj(data.edge_index)\n",
    "\n",
    "\n",
    "    #GNN to compute transformed node features for pooling (for assignation matrix)\n",
    "    class GNN(torch.nn.Module):\n",
    "        def __init__(self, in_channels, hidden_channels, out_channels,\n",
    "                     normalize=False, lin=True):\n",
    "            super(GNN, self).__init__()\n",
    "            self.conv1 = DenseSAGEConv(in_channels, hidden_channels, normalize)\n",
    "            self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "            self.conv2 = DenseSAGEConv(hidden_channels, hidden_channels, normalize)\n",
    "            self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "            self.conv3 = DenseSAGEConv(hidden_channels, out_channels, normalize)\n",
    "            self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "            if lin is True:\n",
    "                self.lin = torch.nn.Linear(\n",
    "                    2 * hidden_channels + out_channels,\n",
    "                    out_channels\n",
    "                )\n",
    "            else:\n",
    "                self.lin = None\n",
    "\n",
    "        def bn(self, i, x):\n",
    "            batch_size, num_nodes, num_channels = x.size()\n",
    "            x = x.view(-1, num_channels)\n",
    "            x = getattr(self, 'bn{}'.format(i))(x)\n",
    "            x = x.view(batch_size, num_nodes, num_channels)\n",
    "            return x\n",
    "\n",
    "        def forward(self, x, adj, mask=None):\n",
    "            x0 = x\n",
    "            if pooling == 'diffpool':\n",
    "                x1 = self.bn(1, F.relu(self.conv1(x0, adj, mask)))\n",
    "                x2 = self.bn(2, F.relu(self.conv2(x1, adj, mask)))\n",
    "                x3 = self.bn(3, F.relu(self.conv3(x2, adj, mask)))\n",
    "            elif pooling == 'mincut':\n",
    "                x1 = F.relu(self.conv1(x0, adj, mask))\n",
    "                x2 = F.relu(self.conv2(x1, adj, mask))\n",
    "                x3 = F.relu(self.conv3(x2, adj, mask))\n",
    "            x = torch.cat([x1, x2, x3], dim=-1)\n",
    "            if self.lin is not None:\n",
    "                x = F.relu(self.lin(x))\n",
    "            return x\n",
    "\n",
    "    #Net to compute pooling in an unsupervised manner\n",
    "    class Net(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.gnn1_pool = GNN(data.num_node_features, 64, num_clusters)\n",
    "            self.gnn1_embed = GNN(data.num_node_features, 64, 64, lin=False)\n",
    "            self.pooling = pooling_selector[pooling]\n",
    "\n",
    "        def forward(self, x, adj, mask=None):\n",
    "            s = self.gnn1_pool(x, adj, mask)\n",
    "            x = self.gnn1_embed(x, adj, mask)\n",
    "            x, adj, l1, e1 = self.pooling(x, adj, s, mask)\n",
    "            return torch.softmax(s, dim=-1), l1, e1, adj # returns assignation matrix, and auxiliary losses and new adj matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###########\n",
    "    #Training\n",
    "    ###########\n",
    "    #Optimizer, model\n",
    "    model = Net().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    #Trains using auxiliary losses only\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        s, l1, e1, adj = model(data.x, data.adj)\n",
    "        loss = l1 + e1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    #Test is the validation loss\n",
    "    @torch.no_grad()\n",
    "    def test():\n",
    "        model.eval()\n",
    "        s, l1, e1, adj = model(data.x, data.adj)\n",
    "        loss = l1 + e1\n",
    "        return loss\n",
    "\n",
    "    log_handle = open(osp.join(outdir, 'log.txt'), 'w')\n",
    "    best_val_loss = best_test_loss = np.inf\n",
    "    for epoch in range(1, 251):\n",
    "        train_loss = train(epoch)\n",
    "        val_loss = test()\n",
    "        test_loss = test()\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_test_loss = test_loss\n",
    "            save_best_model(model, optimizer, epoch, outdir)\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.8f}, '\n",
    "              f'Val loss: {val_loss:.4f}, Test loss: {test_loss:.4f}')\n",
    "        log_handle.write(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.8f}, '\n",
    "              f'Val loss: {val_loss:.4f}, Test loss: {test_loss:.4f}\\n')\n",
    "\n",
    "    print(f'Best Val loss: {best_val_loss:.4f}, Test loss: {best_test_loss:.4f}')\n",
    "    log_handle.write(f'\\nBest Val loss: {best_val_loss:.4f}, Test loss: {best_test_loss:.4f}\\n')\n",
    "    log_handle.close()\n",
    "\n",
    "\n",
    "\n",
    "    #Loads best model\n",
    "    model = load_best_model(model, outdir)\n",
    "    model.eval()\n",
    "    node_assignation = model(data.x, data.adj)[0].max(-1)[1].detach().cpu().numpy().squeeze()\n",
    "    adj_matrix = model(data.x, data.adj)[3].detach().cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    #Computes node clustering and adj matrices and saves them\n",
    "    pd.DataFrame(node_assignation).transpose().to_csv(osp.join(outdir, 'node_assignation.csv'), header = None, index=False)\n",
    "    pd.DataFrame(adj_matrix).to_csv(osp.join(outdir, 'adj_matrix.csv'), header = None, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d578c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(node_assignation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1084c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(data.x, data.adj)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30777af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "math.ceil(0.05 * data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ee22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "27 86"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_torchg_172",
   "language": "python",
   "name": "p3_6_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
